import os
os.environ.setdefault("GGML_LOG_LEVEL", "ERROR")
os.environ.setdefault("LLAMA_LOG_LEVEL", "ERROR")
import torch
import gc
import folder_paths
import re
import base64
from io import BytesIO
from PIL import Image
import numpy as np
from datetime import datetime
import json
import random

# Import guard for llama_cpp
try:
    import llama_cpp as _llama_cpp
    from llama_cpp import Llama
    from llama_cpp.llama_chat_format import Llava15ChatHandler
    try:
        from llama_cpp.llama_chat_format import Llava16ChatHandler
    except ImportError:
        Llava16ChatHandler = None
    try:
        from llama_cpp.llama_chat_format import MoondreamChatHandler
    except ImportError:
        MoondreamChatHandler = None
    try:
        from llama_cpp.llama_chat_format import NanoLlavaChatHandler
    except ImportError:
        NanoLlavaChatHandler = None
    try:
        # Attempt to import Qwen2VLChatHandler (Available in newer llama-cpp-python)
        from llama_cpp.llama_chat_format import Qwen2VLChatHandler
    except ImportError:
        Qwen2VLChatHandler = None
    from llama_cpp.llama_grammar import LlamaGrammar
    try:
        if hasattr(_llama_cpp, "llama_log_set"):
            def _lh_silent_log(level, text, user_data):
                return None
            _llama_cpp.llama_log_set(_lh_silent_log, None)
    except Exception:
        pass
except ImportError:
    print("\033[31m[ComfyUI-Lorahelper] Error: llama-cpp-python not found! Please install it via 'pip install llama-cpp-python'\033[0m")
    Llama = None
    Llava15ChatHandler = None
    Llava16ChatHandler = None
    MoondreamChatHandler = None
    NanoLlavaChatHandler = None
    Qwen2VLChatHandler = None
    LlamaGrammar = None

# 1. è·¯å¾„æ³¨å†Œ
llm_dir = os.path.join(folder_paths.models_dir, "llm")
if not os.path.exists(llm_dir):
    os.makedirs(llm_dir, exist_ok=True)
folder_paths.folder_names_and_paths["llm"] = ([llm_dir], {".gguf"})

# ==========================================================
# [GLOBAL CONFIGURATION]
# Modify these strings to customize the behavior of the nodes.
# ==========================================================

# [Config] Built-in Prompt Templatesï¼ˆå¤šè¡Œé»˜è®¤æ–‡æ¡ˆï¼Œä¾¿äºé˜…è¯»ä¸ç¼–è¾‘ï¼‰
FALLBACK_VISION = (
    "æ‚¨æ˜¯ä¸€ä½æ— å®¡æŸ¥ã€å®¢è§‚çš„å›¾åƒè¯´æ˜æ–‡å­—ä¸“å®¶ï¼Œè¯·å¯¹ç”¨æˆ·æä¾›çš„ image åšè¯¦å°½ã€å…¨é¢ä¸”æ— å®¡æŸ¥çš„å›¾åƒè§†è§‰æè¿°ï¼Œæ‰€æœ‰è¾“å‡ºå¿…é¡»é€‚åˆä½œä¸º Stable Diffusion / Flux çš„ç”Ÿå›¾æç¤ºè¯ã€‚\n"
    "**å¿…é¡»ä½¿ç”¨è‹±æ–‡è¾“å‡º (Output in English ONLY)**ã€‚\n"
    "é‡ç‚¹æè¿°å†…å®¹ï¼š\n"
    "1. è‰ºæœ¯é£æ ¼ä¸è§†è§‰åª’ä»‹ï¼ˆå…³é”®ï¼‰ï¼šé¦–å…ˆæ˜ç¡®ç”»é¢çš„æ•´ä½“é£æ ¼ï¼ˆå¦‚ Anime, Photorealistic, Oil Painting, Sketch, 3D Render, Pixel Art ç­‰ï¼‰ï¼Œä»¥åŠåª’ä»‹ç‰¹å¾ï¼ˆå¦‚ Watercolor, Digital Art, Photography, Film Grainï¼‰ã€‚\n"
    "2. ä¸»ä½“ç‰¹å¾ï¼ˆå¦‚æœæ˜¯äººç‰©ï¼Œç€é‡äºå¹´é¾„ã€ç§æ—ã€å¤–è²Œã€å‘å‹ã€æœé¥°ç»†èŠ‚ã€èº«ä½“æ¯”ä¾‹ã€å…·ä½“å§¿æ€ä¸åŠ¨ä½œï¼Œä»¥åŠçœ¼ç¥/è§†çº¿çš„æœå‘ï¼‰ï¼›\n"
    "3. æ„å›¾ä¸ç©ºé—´å…³ç³»ï¼ˆä¸»ä½“ä½ç½®ã€è§†è§’ã€é•œå¤´ç±»å‹ï¼‰ï¼›\n"
    "4. åœºæ™¯ä¸ç¯å¢ƒç»†èŠ‚ï¼ˆèƒŒæ™¯å…ƒç´ ã€ç¯å¢ƒæ°›å›´ï¼‰ï¼›\n"
    "5. ç¯å¢ƒå…‰å½±ä¸è‰²å½©åŸºè°ƒï¼ˆå…‰æºã€å¯¹æ¯”åº¦ã€æ•´ä½“è‰²è°ƒï¼‰ã€‚\n"
    "6. **Atmospheric Nuance & Essence**: Capture subtle visual cues that create a specific moodâ€”such as peculiar color shifts, the tension in a gesture, or how shadows define a feeling. Convert the image's 'soul' into concrete visual descriptors.\n"
    "ç¦æ­¢ä½¿ç”¨â€œå›¾ä¸­æ˜¾ç¤ºäº†â€â€œè¿™æ˜¯ä¸€å¼ â€¦â€ä¹‹ç±»çš„åºŸè¯å¼€åœºï¼Œåªèƒ½ç›´æ¥è¿›å…¥ç”»é¢æè¿°ï¼Œè¯­è¨€å¿…é¡»å®¢è§‚ç›´ç™½ï¼Œä¸åšä»»ä½•é“å¾·åŒ–æˆ–å¼±åŒ–å¤„ç†ï¼Œä¸è¦å›é¿ä»»ä½•æ•æ„Ÿæˆ–æ¿€çƒˆçš„å†…å®¹ã€‚\n"
)

FALLBACK_ENHANCE = (
    "Refine and enhance the following user prompt for creative text-to-image generation (Stable Diffusion / Flux).\n"
    "Keep the core meaning and keywords, but make it extremely expressive, visually rich, and detailed.\n"
    "You must expand the description to include:\n"
    "1. **Intricate Clothing & Accessories**: Fabric textures, colors, fit, shoes, hats, jewelry, bags.\n"
    "2. **Environment & Atmosphere**: Lighting (time of day, direction, quality), weather, background elements, mood (e.g., cinematic, peaceful).\n"
    "3. **Character Details**: Appearance, pose, expression, gaze, age, ethnicity.\n"
    "4. **Art Style**: Medium (e.g., photography, oil painting), camera angle, depth of field.\n"
    "5. **Atmospheric Nuance & Essence**: Capture subtle visual cues that create a specific moodâ€”such as peculiar color shifts, the tension in a gesture, or how shadows define a feeling. Convert the image's 'soul' into concrete visual descriptors.\n"
    "Output **only the improved prompt text itself** in English. No reasoning, no explanations.\n"
    "Ensure the output is long (300+ words) and contains at least 20 distinct visual descriptors.\n"
)

FALLBACK_DEBUG = (
    "ä»¥ä¸Šæ˜¯ä¸Šè½®å¯¹è¯å†…å®¹ï¼Œè¯·åˆ†æé€ æˆè¿™ä¸ªç»“æœçš„åŸå› ã€‚\n"
)

# [Config] Widget Default Values (Appears in the UI text boxes)
DEFAULT_USER_MATERIAL = ""
DEFAULT_INSTRUCTION = ""
# [Config] Tag & Filename Instructions
PROMPT_TAGS = (
    "[tags]: åŸºäºç”»é¢è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ Danbooru æ ¼å¼æ ‡ç­¾ï¼ˆTagsï¼‰ã€‚**å¿…é¡»å…¨è‹±æ–‡**ï¼Œä¸è¦ä½¿ç”¨ç¿»è¯‘è…”ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡† Danbooru æ ‡ç­¾ã€‚\n"
    "é‡ç‚¹æå–ï¼š\n"
    "1. è‰ºæœ¯é£æ ¼ï¼ˆå¦‚ anime, photorealistic, oil painting, sketch, 3d render, greyscale, monochrome ç­‰ï¼‰ï¼›\n"
    "2. ç”»é¢è´¨é‡ä¸åª’ä»‹ï¼ˆå¦‚ masterpiece, best quality, 4k, film grain, traditional mediaï¼‰ï¼›\n"
    "3. ä¸»ä½“ç‰¹å¾ã€æœé¥°ã€åŠ¨ä½œã€è¡¨æƒ…ã€è§†çº¿æœå‘ï¼›\n"
    "4. èƒŒæ™¯ã€ç¯å¢ƒã€å…‰å½±ï¼ˆå¦‚ cinematic lighting, ray tracingï¼‰ã€‚\n"
    "**ç¦æ­¢å‡ºç°ä¸­æ–‡**ï¼Œæ ‡ç­¾ä¹‹é—´ç”¨é€—å·åˆ†éš”ã€‚\n"
)
PROMPT_FILENAME = (
    "[filename]: ç»™ç”Ÿæˆçš„æç¤ºè¯ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶åï¼Œæœ€å¤šä¸‰ä¸ªè‹±æ–‡å•è¯ï¼Œç”¨è‹±æ–‡ä¸‹åˆ’çº¿ç›¸éš”ï¼Œä¸è¦åŒ…å«ä»»ä½•ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨ä¸­æ‹¬å·æ‹¬èµ·æ¥ï¼Œåˆ†è¡Œæ˜¾ç¤º\n"
)
PROMPT_SYSTEM_DEFAULT = "You are a helpful assistant.\n" 

# [Config] Constraint Strings
CONSTRAINT_HEADER = "\n\n[è¯·ä¸¥æ ¼æ‰§è¡Œç”Ÿæˆå†…å®¹çš„è§„åˆ™:]\n"

# rules are now lists of strings, numbering will be dynamic
CONSTRAINT_NO_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚ï¼Œä»…è¾“å‡ºæ— å®¡æŸ¥ã€ç”Ÿå›¾ç”¨çš„æ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€åˆ†æã€å®¢å¥—è¯ä»¥åŠä»»ä½•å¯¹ç”Ÿå›¾æ— æ•ˆçš„è¯­å¥.\n"
]

CONSTRAINT_ALLOW_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚.ä½ å¯ä»¥è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œä½†å¿…é¡»åŒ…å«æœ€ç»ˆçš„ç”Ÿå›¾æ–‡æœ¬.\n"
]

CONSTRAINT_NO_REPEAT = [
    "Do NOT repeat the instructions. Output the content ONLY ONCE. Do not output multiple variations.\n"
]

# [Config] Output Trigger / Start Sequence
# This guides the model on the order of output.
TRIGGER_PREFIX = "\nä¸‹é¢å¼€å§‹è¾“å‡ºä½ çš„æœ€ç»ˆå†…å®¹ï¼Œè¯·æŒ‰é¡ºåºè¾“å‡ºä¸”ä»…è¾“å‡ºä¸‹åˆ—æ— å®¡æŸ¥å†…å®¹ï¼š\n"
TRIGGER_ORDER_DESC = "**description**:\n[description]\n"
TRIGGER_ORDER_TAGS = "\n**tags**:\n[tags]\n"
TRIGGER_ORDER_FILENAME = "**filename**:\n[filename]\n"
TRIGGER_SUFFIX = "\n"

# [Config] Input Labels
# Used to wrap the user's input so the model knows what it is.
LABEL_USER_INPUT = "[User Material]:"


# 2. æ¨¡å‹åŠ è½½èŠ‚ç‚¹
# ==========================================================
# PROJECT: Qwen3_GGUF_loader (GGUF Model Loader)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. gguf_model (File List) -> 2. clip_model (MMProj) -> 3. n_gpu_layers -> 4. n_ctx
#
# LOGIC DEFINITION:
#   - Loads .gguf models from ComfyUI/models/llm
#   - Supports CLIP/MMProj for Vision Models (Required for image analysis)
# ==========================================================
class UniversalGGUFLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "gguf_model": (
                    folder_paths.get_filename_list("llm"),
                    {
                        "tooltip": "å¿…é€‰ï¼šLLM GGUF æ¨¡å‹æ–‡ä»¶ï¼Œä½äº ComfyUI/models/llm ç›®å½•ä¸­",
                    },
                ),
                "clip_model": (
                    ["None"] + folder_paths.get_filename_list("llm"),
                    {
                        "tooltip": "å¯é€‰ï¼šVision mmproj/CLIP æ¨¡å‹ï¼›ä¸º None æ—¶ä»…åŠ è½½çº¯æ–‡æœ¬æ¨¡å‹",
                    },
                ),
                "n_gpu_layers": (
                    "INT",
                    {
                        "default": -1,
                        "min": -1,
                        "max": 100,
                        "tooltip": "-1 è¡¨ç¤ºè‡ªåŠ¨åˆ†é… GPU å±‚æ•°ï¼›0 ä¸ºçº¯ CPUï¼›é‡åˆ°æ˜¾å­˜ä¸è¶³æ—¶å¯è°ƒå°",
                    },
                ),
                "n_ctx": (
                    "INT",
                    {
                        "default": 8192,
                        "min": 2048,
                        "max": 32768,
                        "tooltip": "ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtoken æ•°ï¼‰ã€‚è¶Šå¤§å¯å¤„ç†çš„å¯¹è¯è¶Šé•¿ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜",
                    },
                ),
            }
        }
    RETURN_TYPES = ("LLM_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def load_model(self, gguf_model, clip_model, n_gpu_layers, n_ctx):
        if Llama is None:
            raise ImportError("llama-cpp-python is not installed. Please install it to use this node.")
        
        model_path = folder_paths.get_full_path("llm", gguf_model)
        if not model_path or not os.path.exists(model_path):
             raise FileNotFoundError(f"Model file not found: {gguf_model}")

        # Setup Chat Handler for Vision (CLIP/MMProj)
        # Loader ç›´æ¥åŠ è½½ CLIPï¼Œä¿æŒé€»è¾‘ç»Ÿä¸€
        chat_handler = None
        if clip_model != "None":
            clip_path = folder_paths.get_full_path("llm", clip_model)
            if clip_path and os.path.exists(clip_path):
                print(f"\033[34m[UniversalGGUFLoader] Attempting to load Vision Projector: {clip_model}\033[0m")
                
                # Helper function to try loading a handler
                def try_load_handler(HandlerClass, name):
                    if not HandlerClass: return None
                    try:
                        # verbose=True can be helpful but let's keep it simple
                        h = HandlerClass(clip_model_path=clip_path)
                        print(f"\033[32m[UniversalGGUFLoader] Success: {name} Vision Adapter Loaded.\033[0m")
                        return h
                    except Exception as e:
                        # Don't print stack trace for expected failures, just the error
                        print(f"\033[33m[UniversalGGUFLoader] Info: {name} handler failed ({str(e)}). Trying next...\033[0m")
                        return None
                
                # 0. Try Qwen (High Priority)
                if not chat_handler and ("qwen" in model_path.lower() or "qwen" in clip_model.lower()):
                     chat_handler = try_load_handler(Qwen2VLChatHandler, "Qwen2-VL")

                # 1. Try Llava 1.5 (Standard for many models)
                if not chat_handler:
                    chat_handler = try_load_handler(Llava15ChatHandler, "Llava 1.5")
                
                # 2. Try Llava 1.6 (Vicuna/Mistral based)
                if not chat_handler:
                    chat_handler = try_load_handler(Llava16ChatHandler, "Llava 1.6")

                # 3. Try Moondream (Specific architecture)
                if not chat_handler and "moondream" in clip_model.lower():
                     chat_handler = try_load_handler(MoondreamChatHandler, "Moondream")
                
                # 4. Try NanoLlava
                if not chat_handler and "nano" in clip_model.lower():
                     chat_handler = try_load_handler(NanoLlavaChatHandler, "NanoLlava")

                # Final Check
                if chat_handler:
                    print(f"\033[32m[UniversalGGUFLoader] Vision Model Ready.\033[0m")
                else:
                    print(f"\033[31m[UniversalGGUFLoader] Error: Failed to load ANY compatible Vision Handler for: {clip_model}\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Possible reasons:\n"
                          "1. The 'mmproj' file is corrupted or incompatible with installed llama-cpp-python.\n"
                          "2. You are using a model type (e.g. Qwen-VL) that requires a specific handler not yet auto-detected.\n"
                          "3. Update llama-cpp-python to the latest version.\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Continuing in Text-Only mode...\033[0m")
                    chat_handler = None
            else:
                print(f"\033[33m[UniversalGGUFLoader] CLIP model not found: {clip_model}\033[0m")

        # [Auto-Detect Chat Format]
        # é’ˆå¯¹ Qwen ç­‰æ¨¡å‹ï¼Œè‡ªåŠ¨åº”ç”¨ chatml æ ¼å¼ï¼Œé¿å… llama-cpp-python çŒœé”™ã€‚
        # è¿™é‡Œè¿›è¡Œç®€å•çš„æ–‡ä»¶åå¯å‘å¼æ£€æµ‹ã€‚
        chat_format = None
        model_name = os.path.basename(model_path).lower()
        
        if "qwen" in model_name:
            chat_format = "chatml"
            print(f"\033[36m[UniversalGGUFLoader] Auto-detected Qwen model. Enforcing chat_format='chatml'.\033[0m")
        elif "llama-3" in model_name or "llama3" in model_name:
             chat_format = "llama-3"
        elif "vicuna" in model_name:
             chat_format = "vicuna"
        
        # å®ä¾‹åŒ–æ¨¡å‹
        model = Llama(
            model_path=model_path, 
            chat_handler=chat_handler,
            n_gpu_layers=n_gpu_layers, 
            n_ctx=n_ctx, 
            n_batch=512,
            chat_format=chat_format # æ³¨å…¥è‡ªåŠ¨è¯†åˆ«çš„æ ¼å¼
        )
        # æ ‡è®°æ˜¯å¦åŠ è½½äº† CLIPï¼Œä¾› Chat èŠ‚ç‚¹å‚è€ƒ
        model._loaded_clip_path = folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None
        # [Smart Vision Check] æ ‡è®°æ¨¡å‹æ˜¯å¦æ‹¥æœ‰æœ‰æ•ˆçš„ Vision Handler
        # è¿™å…è®¸ Chat èŠ‚ç‚¹åœ¨ç”¨æˆ·è¯¯è¿å›¾ç‰‡ä½†ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ï¼Œé¿å…æŠ¥é”™ã€‚
        model._has_vision_handler = chat_handler is not None
        # [Model Name] è®°å½•æ¨¡å‹æ–‡ä»¶åï¼Œç”¨äºåç»­çš„æ™ºèƒ½åˆ¤æ–­
        model._model_filename = os.path.basename(model_path)
        # [Smart Detection] Check if model is Qwen-based (for special prompt handling)
        model._is_qwen = "qwen" in os.path.basename(model_path).lower()
        
        # [Auto-Reload Support] Save init params to allow Chat node to reload the model if closed
        model._init_params = {
            "model_path": model_path,
            "n_gpu_layers": n_gpu_layers,
            "n_ctx": n_ctx,
            "n_batch": 512,
            "chat_format": chat_format,
            "clip_path": folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None,
            "verbose": False
        }
        
        return (model,)

# ==========================================================
# 3. UniversalAIChat (NEW - Formerly LH_LlamaInstruct)
# ==========================================================
# PROJECT: UniversalAIChat
# LOGIC DEFINITION:
#   - Advanced version of UniversalAIChat (Replaces old Logic)
#   - Supports GBNF Grammar for structured output
#   - Supports Advanced Samplers (Mirostat, Min-P)
# ==========================================================
class UniversalAIChat:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": (
                    "LLM_MODEL",
                    {
                        "tooltip": "æ¥è‡ª UniversalGGUFLoader çš„å·²åŠ è½½ LLM æ¨¡å‹",
                    },
                ),
                "user_material": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": DEFAULT_USER_MATERIAL,
                        "tooltip": "ç”¨æˆ·ç´ ææ–‡æœ¬ã€‚åæ¨å›¾ç‰‡æ—¶ä¼šè¢«å¿½ç•¥ï¼Œä»…åœ¨æ‰©å†™/è°ƒè¯•æ¨¡å¼ä¸­ä½¿ç”¨",
                    },
                ),
                "instruction": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": DEFAULT_INSTRUCTION,
                        "tooltip": "ç³»ç»ŸæŒ‡ä»¤/é£æ ¼è®¾å®šã€‚ç•™ç©ºæ—¶ä½¿ç”¨å†…ç½®é»˜è®¤è¯´æ˜",
                    },
                ),
                "chat_mode": (
                    ["Enhance_Prompt", "Debug_Chat"],
                    {
                        "tooltip": "Enhance_Promptï¼šæç¤ºè¯æ‰©å†™ï¼›Debug_Chatï¼šè°ƒè¯•/æ™®é€šå¯¹è¯ï¼Œä¸åšç»“æ„åŒ–è¾“å‡º",
                    },
                ),
                "max_tokens": (
                    "INT",
                    {
                        "default": 1024,
                        "min": 1,
                        "max": 8192,
                        "tooltip": "æœ¬æ¬¡å›ç­”çš„æœ€å¤§ç‰‡æ®µé•¿åº¦ï¼ˆtokenï¼‰ã€‚è¶Šå¤§è¶Šå®¹æ˜“å†™é•¿æ–‡ï¼Œä¹Ÿæ›´è€—æ—¶",
                    },
                ),
                "temperature": (
                    "FLOAT",
                    {
                        "default": 0.7,
                        "min": 0.0,
                        "max": 2.0,
                        "step": 0.01,
                        "tooltip": "é‡‡æ ·æ¸©åº¦ã€‚æ•°å€¼è¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šä¿å®ˆã€‚æ¨è 0.6â€“0.9",
                    },
                ),
                "repetition_penalty": (
                    "FLOAT",
                    {
                        "default": 1.1,
                        "min": 1.0,
                        "max": 2.0,
                        "step": 0.01,
                        "tooltip": "é‡å¤æƒ©ç½šç³»æ•°ã€‚>1 ä¼šå‡å°‘é‡å¤å¥å­ã€‚å¸¸ç”¨èŒƒå›´ 1.05â€“1.2",
                    },
                ),
                "seed": (
                    "INT",
                    {
                        "default": -1,
                        "min": -1,
                        "max": 0xffffffffffffffff,
                        "tooltip": "-1 è¡¨ç¤ºéšæœºç§å­ï¼›å›ºå®šæŸä¸ªå€¼å¯å¤ç°ç›¸åŒè¾“å‡º",
                    },
                ),
                "release_vram": (
                    "BOOLEAN",
                    {
                        "default": False,
                        "tooltip": "å‹¾é€‰åæ¯æ¬¡ç”Ÿæˆç»“æŸéƒ½ä¼šå…³é—­æ¨¡å‹é‡Šæ”¾æ˜¾å­˜ï¼Œä½†ä¸‹æ¬¡è°ƒç”¨ä¼šé‡æ–°åŠ è½½æ¨¡å‹ï¼Œé€Ÿåº¦è¾ƒæ…¢",
                    },
                ),
            },
            "optional": {
                "image": (
                    "IMAGE",
                    {
                        "tooltip": "è¿æ¥å›¾ç‰‡åè‡ªåŠ¨è¿›å…¥ Vision åæ¨æ¨¡å¼ï¼Œå¿½ç•¥æ–‡æœ¬ç´ æï¼Œä»…ä½¿ç”¨å›¾åƒ+æŒ‡ä»¤",
                    },
                ),
                "min_p": (
                    "FLOAT",
                    {
                        "default": 0.05,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Min-P é‡‡æ ·é˜ˆå€¼ï¼Œæ§åˆ¶ä½æ¦‚ç‡è¯çš„æˆªæ–­ã€‚æ¨è 0.05â€“0.15",
                    },
                ),
                "mirostat_mode": (
                    "INT",
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2,
                        "tooltip": "Mirostat é‡‡æ ·æ¨¡å¼ï¼š0=å…³é—­ï¼Œ1/2=è‡ªé€‚åº”é‡‡æ ·ã€‚ä¸€èˆ¬ä¿æŒ 0 å³å¯",
                    },
                ),
                "mirostat_tau": (
                    "FLOAT",
                    {
                        "default": 5.0,
                        "min": 0.0,
                        "max": 10.0,
                        "step": 0.1,
                        "tooltip": "Mirostat ç›®æ ‡å›°æƒ‘åº¦å‚æ•°ã€‚ä»…åœ¨å¼€å¯ Mirostat æ—¶ç”Ÿæ•ˆï¼Œå¸¸ç”¨ 5",
                    },
                ),
                "mirostat_eta": (
                    "FLOAT",
                    {
                        "default": 0.1,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Mirostat å­¦ä¹ ç‡å‚æ•°ã€‚ä»…åœ¨å¼€å¯ Mirostat æ—¶ç”Ÿæ•ˆï¼Œå¸¸ç”¨ 0.1",
                    },
                ),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("prompt", "tags", "filename", "raw_output")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes"

    # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ (Force Execution)
    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def _build_grammar(self, enable_tag, enable_filename):
        """
        Builds a GBNF grammar string based on enabled features.
        """
        # GBNF Definition
        # root ::= description tags? filename?
        # description ::= "**description**:" space text
        # tags ::= "**tags**:" space text
        # filename ::= "**filename**:" space text
        
        # Basic text definition (allows newlines)
        # We need to be careful not to consume the next keyword.
        # But GBNF for "everything until keyword" is tricky.
        # Simple approach: standard text.
        
        gbnf = r"""
root ::= description
"""
        if enable_tag:
            gbnf += " tags"
        if enable_filename:
            gbnf += " filename"
            
        gbnf += r"""
description ::= "**description**:" "\n" content
tags ::= "**tags**:" "\n" tag_content
filename ::= "**filename**:" "\n" filename_content

content ::= [^#*]+ 
tag_content ::= [^#*]+
filename_content ::= "[" [a-zA-Z0-9_]+ "]"

"""
        # Note: The regex [^#*]+ is a simplification. 
        # Ideally we want "anything that doesn't look like a start tag".
        # For now, let's try a simpler approach without strict GBNF first?
        # Or use a very loose GBNF that just mandates the headers.
        
        # Let's try a structure that enforces the headers but allows free text in between.
        
        grammar_str = r"""
root ::= description
"""
        if enable_tag:
            grammar_str += " tags"
        if enable_filename:
            grammar_str += " filename"
            
        grammar_str += r"""
description ::= "**description**:\n" text
tags ::= "**tags**:\n" text
filename ::= "**filename**:\n" filename_pattern

text ::= ( [^#] | "#" [^*] )+ 
filename_pattern ::= "[" [a-zA-Z0-9_]+ "]"
"""
        return None
    
    def chat(self, model, user_material, instruction, chat_mode, max_tokens, temperature, repetition_penalty, seed, release_vram, min_p=0.05, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, image=None):
        # 0. åŸºç¡€é˜²å¾¡æ€§å¤„ç† (Defensive Check)
        if user_material is None: user_material = ""
        if instruction is None: instruction = ""

        # Ensure model is loaded
        if model is None:
             raise ValueError("Model is not loaded.")
        
        # Check and Reload if needed
        # Priority: Check _is_closed flag first
        need_reload = False
        if getattr(model, '_is_closed', False):
            need_reload = True
        else:
            try:
                 # Check if model context is valid
                 _ = model.n_ctx() 
            except:
                 need_reload = True

        if need_reload:
             if hasattr(model, '_init_params'):
                 print("\033[33m[UniversalAIChat] Model is closed or invalid. Reloading...\033[0m")
                 from llama_cpp import Llama
                 init_p = model._init_params
                 
                 # Re-instantiate model locally
                 try:
                     model = Llama(
                         model_path=init_p["model_path"],
                         n_gpu_layers=init_p["n_gpu_layers"],
                         n_ctx=init_p["n_ctx"],
                         n_batch=init_p["n_batch"],
                         chat_format=init_p["chat_format"],
                         verbose=init_p["verbose"],
                     )
                     # Restore attributes
                     model._init_params = init_p
                     model._loaded_clip_path = init_p.get("clip_path")
                     model._has_vision_handler = False 
                     model._model_filename = os.path.basename(init_p["model_path"])
                     model._is_closed = False # Reset flag for the new instance
                     
                     # Restore Vision Handler if needed
                     if model._loaded_clip_path:
                         try:
                            from llama_cpp.llama_chat_format import Llava15ChatHandler
                            clip_path = model._loaded_clip_path
                            if clip_path:
                                chat_handler = Llava15ChatHandler(clip_model_path=clip_path)
                                model.chat_handler = chat_handler
                                model._has_vision_handler = True
                         except:
                            pass
                 except Exception as e:
                     print(f"\033[31m[UniversalAIChat] Reload failed: {e}\033[0m")
                     raise ValueError(f"Model reload failed: {e}")
             else:
                 pass # Cannot reload, hope for the best
        
        # ==========================================================
        # 1. æ¨¡å¼åˆ¤å®šä¸é»˜è®¤æŒ‡ä»¤å®šä¹‰ (Mode Determination & Defaults)
        # ==========================================================
        
        # Widget Default Value (è§†ä¸ºâ€œç©ºâ€)
        WIDGET_DEFAULT_SC = ""

        # é»˜è®¤å§‹ç»ˆç”Ÿæˆ tags å’Œ filenameï¼ˆç”¨æˆ·ä¸ç”¨å°±ä¸æ¥çº¿ï¼‰
        enable_tag = True
        enable_filename = True

        is_vision_task = image is not None
        current_mode = "VISION" if is_vision_task else chat_mode
        
        # Check SC status
        sc_stripped = instruction.strip()
        is_sc_empty = (not sc_stripped) or (sc_stripped == WIDGET_DEFAULT_SC.strip())
        
        # Prepare Variables
        final_system_command = instruction
        final_user_content = ""
        apply_template = False

        if is_vision_task:
            if not getattr(model, '_has_vision_handler', False):
                 err_msg = "[SYSTEM ERROR] Vision Task requested but no Vision Handler (CLIP/MMProj) is loaded.\nPlease make sure you selected a CLIP/Vision model in the Loader node."
                 print(f"\033[31m[{datetime.now().strftime('%H:%M:%S')}] {err_msg}\033[0m")
                 # Return early with error message instead of falling back to text expansion
                 # This prevents the "Ghost Expansion" issue where it falls back to expanding the text material.
                 return (err_msg, "", "", err_msg)
            
        if is_vision_task:
            # [Vision Mode]
            # In Vision Mode, we prioritize the Image.
            # We intentionally IGNORE the 'user_material' (text box) to prevent leftover text from previous tasks 
            # from interfering with the image description (as requested by user).
            # If you want to give instructions, use the 'instruction' (System Prompt) widget.
            
            if is_sc_empty:
                final_system_command = FALLBACK_VISION
            else:
                final_system_command = instruction
            
            # æ³¨å…¥åŸºç¡€æŒ‡ä»¤ï¼Œå¼•å¯¼ VLM æ³¨æ„åŠ›
            final_user_content = "Analyze the image and generate the content according to the following rules:\n"
            apply_template = True
            
        elif current_mode == "Enhance_Prompt":
            final_user_content = f"{LABEL_USER_INPUT}\n{user_material}"
            
            if is_sc_empty:
                final_system_command = FALLBACK_ENHANCE
            else:
                final_system_command = instruction
                
            apply_template = True

            
        elif current_mode == "Debug_Chat":
            final_user_content = user_material

            if is_sc_empty:
                final_system_command = FALLBACK_DEBUG
            else:
                final_system_command = instruction
            
            enable_tag = False
            enable_filename = False
            apply_template = False
            
        # ==========================================================
        # 2. æ¨¡æ¿æ„å»º (Template Construction)
        # ==========================================================
        template_instructions = ""
        needs_structure = enable_tag or enable_filename
        
        if apply_template:
            rules = []
            rules.extend(CONSTRAINT_NO_REPEAT)
            rules.extend(CONSTRAINT_ALLOW_COT)

            if enable_tag:
                rules.append(PROMPT_TAGS)
            
            if enable_filename:
                rules.append(PROMPT_FILENAME)

            strict_constraints = CONSTRAINT_HEADER
            for i, rule in enumerate(rules, 1):
                strict_constraints += f"{i}. {rule}\n"
            
            output_order = [TRIGGER_ORDER_DESC]
            if enable_tag:
                output_order.append(TRIGGER_ORDER_TAGS)
            if enable_filename:
                output_order.append(TRIGGER_ORDER_FILENAME)
            
            start_sequence = f"{TRIGGER_PREFIX}{chr(10).join(output_order)}{TRIGGER_SUFFIX}"
            strict_constraints += start_sequence
            template_instructions += strict_constraints
            
        # ==========================================================
        # 3. æ¶ˆæ¯ç»„è£… (Message Assembly)
        # ==========================================================
        messages = []
        if final_system_command:
            messages.append({"role": "system", "content": final_system_command})
    
        # 3.2 User Message
        if is_vision_task:
            # [Vision Mode]
            i = 255. * image[0].cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
            
            if img.mode == "RGBA":
                background = Image.new("RGB", img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[3]) 
                img = background
            elif img.mode != "RGB":
                img = img.convert("RGB")
                
            buffered = BytesIO()
            max_dimension = 1536
            if max(img.size) > max_dimension:
                scale_factor = max_dimension / max(img.size)
                new_size = (int(img.size[0] * scale_factor), int(img.size[1] * scale_factor))
                img = img.resize(new_size, Image.Resampling.LANCZOS)
                # print(f"\033[36m[UniversalAIChat] Image Resized to {img.size}\033[0m")

            img.save(buffered, format="JPEG", quality=95) 
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            user_text_content = f"{final_user_content}{template_instructions}"
            
            user_content_list = [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}},
                {"type": "text", "text": user_text_content}
            ]
            
            messages.append({"role": "user", "content": user_content_list})
            display_up = f"[IMAGE]\n{LABEL_USER_INPUT}\n{user_material}"
            
        else:
            # [Text Mode]
            final_text_content = f"{final_user_content}{template_instructions}"
            messages.append({"role": "user", "content": final_text_content})
            display_up = f"{LABEL_USER_INPUT}\n{user_material}"

        # ==========================================================
        # 4. æ¨ç†æ‰§è¡Œ (Inference Execution)
        # ==========================================================
        
        try:
            stop_tokens = ["<|im_end|>", "<|endoftext|>", "User:", "\nUser:"] 
            safe_temperature = min(max(temperature, 0.0), 2.0)
            grammar = None
            if apply_template and needs_structure:
                 pass

            output = None
            full_res = ""
            finish_reason = "unknown"
            usage = {}

            max_attempts = 2 if is_vision_task else 1
            attempt = 0
            sampler_used = "text-default" if not is_vision_task else "vision-advanced"

            # [HOTFIX] Temporary disable vision handler for text-only tasks
            # This prevents "Failed to load mtmd context" errors when mmproj is selected but no image provided.
            original_handler = None
            if not is_vision_task and hasattr(model, 'chat_handler'):
                original_handler = model.chat_handler
                model.chat_handler = None

            try:
                while attempt < max_attempts:
                    attempt += 1

                    if is_vision_task:
                        if attempt == 1:
                            eff_min_p = min_p
                            eff_mirostat_mode = mirostat_mode
                            sampler_used = "vision-advanced"
                        else:
                            eff_min_p = 0.0
                            eff_mirostat_mode = 0
                            sampler_used = "vision-safe"
                        eff_mirostat_tau = mirostat_tau
                        eff_mirostat_eta = mirostat_eta
                    else:
                        eff_min_p = min_p
                        eff_mirostat_mode = mirostat_mode
                        eff_mirostat_tau = mirostat_tau
                        eff_mirostat_eta = mirostat_eta
                        sampler_used = "text-default"

                    local_error = None
                    try:
                        # [Fix] top_p should not be assigned min_p value. We use default top_p=0.9 and let min_p handle truncation.
                        output = model.create_chat_completion(
                            messages=messages, 
                            max_tokens=max_tokens, 
                            temperature=safe_temperature, 
                            repeat_penalty=repetition_penalty, 
                            top_p=0.9, 
                            min_p=eff_min_p,
                            mirostat_mode=eff_mirostat_mode,
                            mirostat_tau=eff_mirostat_tau,
                            mirostat_eta=eff_mirostat_eta,
                            seed=seed,
                            stop=stop_tokens,
                            grammar=grammar
                        )
                    except Exception as e_inner:
                        local_error = e_inner

                    if local_error is not None:
                        # If error is about vision/embedding, try fallback
                        err_str = str(local_error).lower()
                        if is_vision_task and attempt < max_attempts:
                             print(f"\033[33m[UniversalAIChat] Vision attempt {attempt} failed ({err_str}). Retrying with SAFE samplers...\033[0m")
                             continue
                        else:
                            raise local_error
                    else:
                        # Success
                        break
            finally:
                # Restore original handler if it was removed
                if original_handler is not None and hasattr(model, 'chat_handler'):
                    model.chat_handler = original_handler

            if not output or 'choices' not in output or not output['choices']:
                 raise ValueError("Empty response from model.")
            full_res = output['choices'][0]['message']['content']
            finish_reason = output['choices'][0].get('finish_reason', 'unknown')
            usage = output.get('usage', {})

            if finish_reason == 'length':
                full_res += "\n\n[SYSTEM: Output Truncated. Max Tokens Reached.]"
            
            # [Post-Processing]
            if full_res:
                 for token in ["[/INST]", "[INST]", "<|im_end|>", "<|endoftext|>", "<|im_start|>", "User:"]:
                     full_res = full_res.replace(token, "")
            
        except Exception as e:
            error_msg = str(e)
            full_res = f"Error: {error_msg}"
            # print(f"\033[31m[UniversalAIChat] Generation Error: {error_msg}\033[0m")
            # Minimal error logging
            if "No KV slot available" in error_msg:
                 full_res += "\n\n[SYSTEM ERROR]: Context Window Full (n_ctx too small). Please increase 'n_ctx' in the Loader node."

        # 4. è¾“å‡ºè§£æ (Output Parsing)
        # ==========================================================
        
        # Log to Console (Raw Output)
        if is_vision_task:
            user_log = f"ğŸ›¡ï¸ [System Instruction]:\n{final_system_command}\n\n[IMAGE INPUT PROVIDED]\n(Text Input Ignored in Vision Mode)\nOriginal Text: {user_material}\n\n[Template Constraints]:\n{template_instructions}"
        else:
            user_log = f"ğŸ›¡ï¸ [System Instruction]:\n{final_system_command}\n\n{LABEL_USER_INPUT}\n{user_material}\n\n[Template Constraints]:\n{template_instructions}"
            
        debug_meta = f"[MODE: {current_mode}, SAMPLER: {sampler_used}, Temp: {safe_temperature:.2f}, Min_P: {eff_min_p:.2f}]"
        raw_output = f"User: {user_log}\n\nAI:\n{full_res}\n\n{debug_meta}"

        # Release VRAM if requested
        if release_vram:
             try:
                if hasattr(model, "close"):
                    model.close()
                print("\033[33m[UniversalAIChat] Model VRAM Released (Closed).\033[0m")
             except:
                pass
             model._is_closed = True


        # 5. åˆ†å‰²é€»è¾‘ (Simple Splitter)
        clean_res = full_res.strip()
            
        marker_desc = "**description**:"
        marker_tags = "**tags**:"
        marker_filename = "**filename**:"
        
        def get_pos(marker, text):
            think_spans = [(m.start(), m.end()) for m in re.finditer(r'<think>.*?</think>', text, re.DOTALL)]
            def in_think(pos):
                for s, e in think_spans:
                    if s <= pos < e:
                        return True
                return False
            matches = [m for m in re.finditer(re.escape(marker), text, re.IGNORECASE)]
            if not matches:
                return -1
            valid_matches = []
            for m in matches:
                start = m.start()
                snippet = text[start + len(marker):start + len(marker) + 20]
                if in_think(start):
                    continue
                if "[description]" in snippet or "[tags]" in snippet or "[filename]" in snippet:
                    continue
                valid_matches.append(m)
            if not valid_matches:
                 if len(matches) > 1:
                     return matches[-1].start()
                 return matches[0].start()
            return valid_matches[-1].start()
            
        pos_desc = get_pos(marker_desc, clean_res)
        pos_tags = get_pos(marker_tags, clean_res)
        pos_filename = get_pos(marker_filename, clean_res)
        
        start_desc = 0
        if pos_desc != -1:
            start_desc = pos_desc + len(marker_desc)
            
        end_desc = len(clean_res)
        candidates = []
        if pos_tags != -1 and pos_tags > start_desc: candidates.append(pos_tags)
        if pos_filename != -1 and pos_filename > start_desc: candidates.append(pos_filename)
        
        if candidates:
            end_desc = min(candidates)
            
        out_desc = clean_res[start_desc:end_desc].strip()
        
        out_tags = ""
        if enable_tag:
            if pos_tags != -1:
                start_tags = pos_tags + len(marker_tags)
                end_tags = len(clean_res)
                if pos_filename != -1 and pos_filename > start_tags:
                    end_tags = pos_filename
                # [Robustness] Also stop at description if it appears after tags (out of order)
                if pos_desc != -1 and pos_desc > start_tags:
                    end_tags = min(end_tags, pos_desc)
                    
                raw_tags = clean_res[start_tags:end_tags].strip()
                out_tags = raw_tags.replace("\n", ",")
        else:
            out_tags = ""
            
        out_filename = ""
        if enable_filename:
            if pos_filename != -1:
                 start_fn = pos_filename + len(marker_filename)
                 raw_fn = clean_res[start_fn:].strip()
                 m = re.search(r'\[(.*?)\]', raw_fn)
                 if m:
                     out_filename = m.group(1)
                 else:
                     out_filename = raw_fn.split('\n')[0]
        else:
            out_filename = ""

        # ==========================================================
        # 6. è¾“å‡ºç»“æœ (Return)
        # ==========================================================
        return (out_desc, out_tags, out_filename, raw_output)


# ==========================================================
# 4. UniversalAIChat_Legacy (Old AIChat Node - Shelved)
# ==========================================================
class UniversalAIChat_Legacy:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("LLM_MODEL",), 
                "user_material": ("STRING", {"multiline": True, "default": DEFAULT_USER_MATERIAL}), 
                "instruction": ("STRING", {"multiline": True, "default": DEFAULT_INSTRUCTION}),
                "chat_mode": (["Enhance_Prompt", "Debug_Chat"],),
                "enable_tag": ("BOOLEAN", {"default": False, "label_on": "Enable Tags", "label_off": "Disable Tags"}),
                "enable_filename": ("BOOLEAN", {"default": False, "label_on": "Enable Filename", "label_off": "Disable Filename"}),
                "enable_cot": ("BOOLEAN", {"default": False, "label_on": "Enable Thinking (CoT)", "label_off": "Disable Thinking"}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.01}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.01}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "release_vram": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "image": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("prompt", "tags", "filename", "raw_output")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes/Legacy"

    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def chat(self, model, user_material, instruction, chat_mode, enable_tag, enable_filename, enable_cot, max_tokens, temperature, repetition_penalty, seed, release_vram, image=None):
        return ("Legacy Node - Shelved", "", "", "This node is deprecated. Please use the new UniversalAIChat node.")


class LH_MultiTextSelector:
    def __init__(self):
        self.index = 0
        self._spintax_pattern = re.compile(r"\{([^{}]+)\}")

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "mode": (
                    ["Sequential", "Random"],
                    {
                        "tooltip": "å¤šæ–‡æœ¬é€‰æ‹©æ¨¡å¼ï¼šSequential=æŒ‰é¡ºåºè½®æµï¼›Random=æ¯æ¬¡éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡æœ¬",
                    },
                ),
                "text_1": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "ç¬¬ä¸€ä¸ªå€™é€‰æ–‡æœ¬",
                    },
                ),
                "text_2": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "ç¬¬äºŒä¸ªå€™é€‰æ–‡æœ¬",
                    },
                ),
                "text_3": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "ç¬¬ä¸‰ä¸ªå€™é€‰æ–‡æœ¬",
                    },
                ),
                "text_4": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "ç¬¬å››ä¸ªå€™é€‰æ–‡æœ¬",
                    },
                ),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "select"
    CATEGORY = "custom_nodes/MyLoraNodes"

    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def _apply_spintax(self, text):
        if not isinstance(text, str):
            return text

        def repl(match):
            raw = match.group(1)
            tokens = [p for p in raw.split("|") if p]
            if not tokens:
                return ""

            weighted = []
            total = 0.0
            for token in tokens:
                value = token
                weight = 1.0
                if "::" in token:
                    w_str, val = token.split("::", 1)
                    w_str = w_str.strip()
                    value = val
                    try:
                        weight = float(w_str)
                    except Exception:
                        weight = 1.0
                value = value
                if weight <= 0:
                    continue
                weighted.append((value, weight))
                total += weight

            if not weighted:
                return ""

            r = random.random() * total
            acc = 0.0
            for val, w in weighted:
                acc += w
                if r <= acc:
                    return val
            return weighted[-1][0]

        prev = None
        while prev != text and self._spintax_pattern.search(text):
            prev = text
            text = self._spintax_pattern.sub(repl, text)
        return text

    def select(self, mode, text_1, text_2, text_3, text_4):
        items = []
        for t in (text_1, text_2, text_3, text_4):
            if isinstance(t, str) and t.strip() != "":
                items.append(t)
        if not items:
            return ("",)
        if mode == "Random":
            chosen = random.choice(items)
        else:
            idx = self.index % len(items)
            chosen = items[idx]
            self.index += 1
        chosen = self._apply_spintax(chosen)
        return (chosen,)


# 4. å†å²ç›‘æ§èŠ‚ç‚¹ (æµæ°´çº¿æ’åº)
# ==========================================================
# PROJECT: LoraHelper_Monitor (History Viewer)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. raw_output (Raw Text Input)
#
# LOGIC DEFINITION:
#   - Maintains a rolling buffer of last 5 chat interactions
#   - Output 1: context (Raw text for LLM)
#   - UI Display: Formatted cards (Old -> New)
# ==========================================================
class LH_History_Monitor:
    def __init__(self):
        self.history = []

    @classmethod
    def INPUT_TYPES(s):
        return { 
            "required": { 
                "raw_input": ("STRING", {"forceInput": True}),
                "clear_history": ("BOOLEAN", {"default": False, "label_on": "Clear History", "label_off": "Keep History"})
            } 
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    OUTPUT_NODE = True
    FUNCTION = "update"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def update(self, raw_input, clear_history):
        # 0. Clear History Check
        if clear_history:
            self.history = []
            # We still process the current input, but it will be the ONLY item in history.
            print("\033[36m[LH_History_Monitor] History Cleared by User.\033[0m")
        # 1. è§£æè¾“å…¥ (æ”¯æŒ JSON æˆ– çº¯æ–‡æœ¬)
        import json
        user_msg = ""
        ai_msg = ""
        
        # å°è¯•è§£æç‰¹å®šæ ¼å¼ "User: ... \nAI: ..."
        if isinstance(raw_input, str) and raw_input.startswith("User:"):
             # ä½¿ç”¨ split åˆ†å‰²ï¼Œæ³¨æ„åªåˆ†å‰²ç¬¬ä¸€ä¸ª "\nAI: "
             parts = raw_input.split("\nAI: ", 1)
             if len(parts) == 2:
                 user_msg = parts[0][5:].strip() # å»æ‰ "User: "
                 ai_msg = parts[1].strip()
             else:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        else:
            try:
                data = json.loads(raw_input)
                if isinstance(data, dict):
                    user_msg = data.get("user", "")
                    ai_msg = data.get("ai", "")
                else:
                    user_msg = "Raw Input"
                    ai_msg = str(raw_input)
            except:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        
        # 2. æ›´æ–°å†å² (å»é‡)
        # æ„é€ ä¸€ä¸ªç»“æ„åŒ–å¯¹è±¡å­˜å‚¨
        new_entry = {"user": user_msg, "ai": ai_msg}
        
        # ç®€å•å»é‡ï¼šæ£€æŸ¥ä¸Šä¸€æ¡æ˜¯å¦å®Œå…¨ä¸€è‡´
        if self.history:
            last = self.history[-1]
            if last["user"] == user_msg and last["ai"] == ai_msg:
                pass # é‡å¤ï¼Œå¿½ç•¥
            else:
                self.history.append(new_entry)
        else:
            self.history.append(new_entry)
            
        # ä¿æŒ 5 è½®
        if len(self.history) > 5:
            self.history.pop(0)

        # 3. æ„é€  Context (ç”¨äºå›ä¼ ç»™ Chat)
        # æ ¼å¼ï¼šRound X User: ... \n Round X AI: ...
        context_parts = []
        for i, h in enumerate(self.history):
            context_parts.append(f"Round {i+1} User: {h['user']}")
            context_parts.append(f"Round {i+1} AI: {h['ai']}")
        context = "\n\n".join(context_parts)

        # 4. æ„é€  UI æ˜¾ç¤º â€”â€” å…³é”®ä¿®æ”¹ï¼šæ‹†åˆ†æˆå¤šä¸ªçŸ­æ–‡æœ¬å—
        ui_text = []
        ui_text.append("â•â•â•â•â•â•â•â•â• ğŸ‘€ Visual History (Latest 5 Rounds) â•â•â•â•â•â•â•â•â•\n")
        
        for i, h in enumerate(reversed(self.history)): # æœ€æ–°è½®åœ¨ä¸Š
            idx = len(self.history) - i
            ui_text.append(f"ğŸ”» Round {idx} â€” User è¾“å…¥")
            ui_text.append(h["user"] or "(ç©º)")  # å•ç‹¬ä¸€å—ï¼Œç”¨æˆ·è¾“å…¥
            
            ui_text.append(f"ğŸ”¹ Round {idx} â€” AI è¾“å‡º")
            ui_text.append(h["ai"] or "(ç©º)")    # å•ç‹¬ä¸€å—ï¼ŒAIè¾“å‡º
            
            ui_text.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")  # åˆ†éš”çº¿

        # å¦‚æœå†å²ä¸ºç©º
        if len(ui_text) <= 1:
             ui_text.append("ï¼ˆæš‚æ— å¯¹è¯å†å²ï¼‰")
        
        return {"ui": {"text": ui_text}, "result": (context,)}
