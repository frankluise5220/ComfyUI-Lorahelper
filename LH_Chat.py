import os
import torch
import gc
import folder_paths
import re
import base64
from io import BytesIO
from PIL import Image
import numpy as np

# Import guard for llama_cpp
try:
    from llama_cpp import Llama
    from llama_cpp.llama_chat_format import Llava15ChatHandler
except ImportError:
    print("\033[31m[ComfyUI-Lorahelper] Error: llama-cpp-python not found! Please install it via 'pip install llama-cpp-python'\033[0m")
    Llama = None
    Llava15ChatHandler = None

# 1. è·¯å¾„æ³¨å†Œ
llm_dir = os.path.join(folder_paths.models_dir, "llm")
if not os.path.exists(llm_dir):
    os.makedirs(llm_dir, exist_ok=True)
folder_paths.folder_names_and_paths["llm"] = ([llm_dir], {".gguf"})

# 2. æ¨¡å‹åŠ è½½èŠ‚ç‚¹
# ==========================================================
# PROJECT: Qwen3_GGUF_loader (GGUF Model Loader)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. gguf_model (File List) -> 2. clip_model (MMProj) -> 3. n_gpu_layers -> 4. n_ctx
#
# LOGIC DEFINITION:
#   - Loads .gguf models from ComfyUI/models/llm
#   - Supports CLIP/MMProj for Vision Models (Required for image analysis)
# ==========================================================
class UniversalGGUFLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "gguf_model": (folder_paths.get_filename_list("llm"),),
                "clip_model": (["None"] + folder_paths.get_filename_list("llm"),),
                "n_gpu_layers": ("INT", {"default": -1, "min": -1, "max": 100}),
                "n_ctx": ("INT", {"default": 8192, "min": 2048, "max": 32768}),
            }
        }
    RETURN_TYPES = ("LLM_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def load_model(self, gguf_model, clip_model, n_gpu_layers, n_ctx):
        if Llama is None:
            raise ImportError("llama-cpp-python is not installed. Please install it to use this node.")
        
        model_path = folder_paths.get_full_path("llm", gguf_model)
        if not model_path or not os.path.exists(model_path):
             raise FileNotFoundError(f"Model file not found: {gguf_model}")

        # Setup Chat Handler for Vision (CLIP/MMProj)
        # Loader ç›´æ¥åŠ è½½ CLIPï¼Œä¿æŒé€»è¾‘ç»Ÿä¸€
        chat_handler = None
        if clip_model != "None":
            clip_path = folder_paths.get_full_path("llm", clip_model)
            if clip_path and os.path.exists(clip_path):
                # [Qwen å…¼å®¹æ€§ä¿®å¤]
                # Qwen-VL ç­‰æ–°æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šçš„ Handlerï¼Œæˆ–è€…å¹²è„†ä¸éœ€è¦ LlavaHandler
                # å¦‚æœæ£€æµ‹åˆ°æ˜¯ Qwen ç³»åˆ—æ¨¡å‹ï¼ˆæ ¹æ®æ–‡ä»¶åï¼‰ï¼Œä¸” LlavaHandler å¤±è´¥ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸åŠ è½½ Handler
                # æˆ–è€…æç¤ºç”¨æˆ·ç¡®è®¤æ¨¡å‹ç±»å‹ã€‚
                # ç›®å‰ llama-cpp-python å¯¹ Qwen2-VL çš„æ”¯æŒè¿˜åœ¨å®éªŒé˜¶æ®µã€‚
                # å¦‚æœæ˜¯ ggufï¼Œæœ‰äº›æ¨¡å‹å·²ç»å†…ç½®äº† projectorï¼Œä¸éœ€è¦é¢å¤–çš„ clipã€‚
                
                # å°è¯•åŠ è½½ Handlerï¼Œå¹¶æ•è·é”™è¯¯è€Œä¸å´©æºƒ
                try:
                    if Llava15ChatHandler:
                        chat_handler = Llava15ChatHandler(clip_model_path=clip_path)
                        print(f"\033[32m[UniversalGGUFLoader] Vision Adapter Loaded: {clip_model}\033[0m")
                    else:
                        print("\033[33m[UniversalGGUFLoader] Warning: Llava15ChatHandler missing.\033[0m")
                except Exception as e:
                    print(f"\033[31m[UniversalGGUFLoader] Failed to load CLIP handler (Llava15): {str(e)}\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Attempting to continue without CLIP handler (for models with built-in vision support or incompatible mmproj)...\033[0m")
                    chat_handler = None
            else:
                print(f"\033[33m[UniversalGGUFLoader] CLIP model not found: {clip_model}\033[0m")

        # [Auto-Detect Chat Format]
        # é’ˆå¯¹ Qwen ç­‰æ¨¡å‹ï¼Œè‡ªåŠ¨åº”ç”¨ chatml æ ¼å¼ï¼Œé¿å… llama-cpp-python çŒœé”™ã€‚
        # è¿™é‡Œè¿›è¡Œç®€å•çš„æ–‡ä»¶åå¯å‘å¼æ£€æµ‹ã€‚
        chat_format = None
        model_name = os.path.basename(model_path).lower()
        
        if "qwen" in model_name:
            chat_format = "chatml"
            print(f"\033[36m[UniversalGGUFLoader] Auto-detected Qwen model. Enforcing chat_format='chatml'.\033[0m")
        elif "llama-3" in model_name or "llama3" in model_name:
             chat_format = "llama-3"
        elif "vicuna" in model_name:
             chat_format = "vicuna"
        
        # å®ä¾‹åŒ–æ¨¡å‹
        model = Llama(
            model_path=model_path, 
            chat_handler=chat_handler,
            n_gpu_layers=n_gpu_layers, 
            n_ctx=n_ctx, 
            n_batch=512,
            chat_format=chat_format # æ³¨å…¥è‡ªåŠ¨è¯†åˆ«çš„æ ¼å¼
        )
        # æ ‡è®°æ˜¯å¦åŠ è½½äº† CLIPï¼Œä¾› Chat èŠ‚ç‚¹å‚è€ƒ
        model._loaded_clip_path = folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None
        # [Smart Vision Check] æ ‡è®°æ¨¡å‹æ˜¯å¦æ‹¥æœ‰æœ‰æ•ˆçš„ Vision Handler
        # è¿™å…è®¸ Chat èŠ‚ç‚¹åœ¨ç”¨æˆ·è¯¯è¿å›¾ç‰‡ä½†ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ï¼Œé¿å…æŠ¥é”™ã€‚
        model._has_vision_handler = chat_handler is not None
        # [Model Name] è®°å½•æ¨¡å‹æ–‡ä»¶åï¼Œç”¨äºåç»­çš„æ™ºèƒ½åˆ¤æ–­
        model._model_filename = os.path.basename(model_path)
        
        return (model,)

# 3. æ ¸å¿ƒå¯¹è¯èŠ‚ç‚¹
# ==========================================================
# PROJECT: LoraHelper_Chat (DeepBlue Architecture)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. model (Loader) -> 2. image (Optional)
#   3. context (History/Top) -> 4. user_prompt (Material/UP) -> 5. system_command (Command/SC)
#   6. chat_mode (Logic Switch) -> 7. max_tokens -> 8. temperature
#   9. repetition_penalty -> 10. seed -> 11. release_vram
#
# LOGIC DEFINITION:
#   - user_prompt = Input Material (UP)
#   - system_command = Executive Instructions (SC)
#   - chat_mode = [Enhance_Prompt, Debug_Chat]
# ==========================================================
class UniversalAIChat:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("LLM_MODEL",), 
                "context": ("STRING", {"multiline": True, "default": ""}), 
                "user_prompt": ("STRING", {"multiline": True, "default": "åœ¨æ­¤è¾“å…¥ç´ æå†…å®¹ (UP)..."}), 
                "system_command": ("STRING", {"multiline": True, "default": "ä½ æ˜¯ä¸€ä¸ªAIæç¤ºè¯å¤§å¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\nSECTION 1:\nè¯·ç”¨è¿è´¯çš„è‡ªç„¶è¯­è¨€è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€è¡¨æƒ…ã€å¤´é¥°ã€æœé¥°ã€åŠ¨ä½œã€åœºæ™¯å’Œæ°›å›´ã€‚ä¸è¦ä½¿ç”¨åˆ—è¡¨ï¼ˆä¸å°‘äº300ä¸ªå•è¯ï¼‰ã€‚\nSECTION 2:\nè¯·è¾“å‡ºæ ‡å‡† Danbooru é£æ ¼æ ‡ç­¾ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚èŒƒå›´ï¼š1.ä¸»ä½“ä¸æ•°é‡(å¦‚ 1girl, solo)ï¼›2.å¤–è²Œç‰¹å¾(ä¿ç•™é¢œè‰²/å½¢æ€ï¼Œå¦‚ long hair, blue eyes)ï¼›3.è¡£ç€é…é¥°(å¦‚ white dress, glasses)ï¼›4.åŠ¨ä½œå§¿æ€(å¦‚ sitting, hand on hip)ï¼›5.æ„å›¾è§†è§’(å¦‚ upper body, close-up, from side)ï¼›6.ç¯å¢ƒèƒŒæ™¯ã€‚ç¦æ­¢ï¼šä¸»è§‚è¯„ä»·è¯(beautiful, amazing)åŠæƒé‡è¯­æ³•ã€‚\nSECTION 3:\nç”¨èŒä¸šè§†è§’ä¸ºå†…å®¹å–ä¸€ä¸ªç®€çŸ­çš„è‹±æ–‡æ ‡é¢˜ï¼ˆç”±ä¸‰ä¸ªä»£è¡¨æ€§åè¯ç»„æˆï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰ï¼Œç”¨æ–¹æ‹¬å·æ‹¬èµ·æ¥ï¼Œä¾‹å¦‚ï¼š[woman bed lamp]ã€‚ä¸è¦åŒ…å«åç¼€æˆ–æ•°å­—ã€‚"}),
                "chat_mode": (["Enhance_Prompt", "Debug_Chat"],),
                "enable_tags_extraction": ("BOOLEAN", {"default": False, "label_on": "Enable Tags", "label_off": "Disable Tags"}),
                "enable_filename_extraction": ("BOOLEAN", {"default": False, "label_on": "Enable Filename", "label_off": "Disable Filename"}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.01}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.01}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "release_vram": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "image": ("IMAGE",), 
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("description", "tags", "filename", "chat_history")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes"

    # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ (Force Execution)
    # é˜²æ­¢ ComfyUI å› ä¸ºè¾“å…¥æœªå˜ï¼ˆå¦‚å›ºå®š Seedï¼‰è€Œè·³è¿‡æ‰§è¡Œï¼Œå¯¼è‡´ç”¨æˆ·ä»¥ä¸ºâ€œæ²¡ååº”â€
    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def chat(self, model, context, user_prompt, system_command, chat_mode, enable_tags_extraction, enable_filename_extraction, max_tokens, temperature, repetition_penalty, seed, release_vram, image=None):
        # 0. åŸºç¡€é˜²å¾¡æ€§å¤„ç† (Defensive Check)
        # ç¡®ä¿è¾“å…¥ä¸ä¸º Noneï¼Œå³ä½¿ ComfyUI ä¼ äº†ç©ºå€¼
        if user_prompt is None: user_prompt = ""
        if system_command is None: system_command = ""
        if context is None: context = ""
        
        # 1. éšå½¢åæ¨æ¨¡å¼åˆ¤å®š (Implicit Vision Mode)
        # é€»è¾‘ï¼š
        # A. å¿…é¡»æœ‰å›¾ç‰‡è¾“å…¥ (image is not None)
        # B. æ¨¡å‹å¿…é¡»æ”¯æŒè§†è§‰ (model._has_vision_handler is True)
        
        has_vision_handler = getattr(model, '_has_vision_handler', False)
        
        # [Strict Logic per User Request]
        # 1. æ¥ image ä»ç„¶æ˜¯æœ€é«˜ä¼˜å…ˆçº§ï¼Œæ¥äº†å°±åæ¨ã€‚
        # 2. å¦‚æœæ²¡æ¥ imageï¼Œå°± enhance (é™¤éè°ƒæˆ debug æ¨¡å¼)ã€‚
        
        is_vision_task = image is not None
        
        if is_vision_task and not has_vision_handler:
             # ç”¨æˆ·è¿äº† imageï¼Œä½†æ¨¡å‹ä¸æ”¯æŒ
             print("\033[31m[UniversalAIChat] CRITICAL WARNING: Image input detected but model has no Vision Handler!\033[0m")
             print("\033[33m[UniversalAIChat] System will attempt to run in Text-Only mode, but results may be unexpected as Vision Logic was requested.\033[0m")
             pass
        
        # æ™ºèƒ½å¤„ç†é»˜è®¤å ä½ç¬¦
        if user_prompt.strip() == "åœ¨æ­¤è¾“å…¥ç´ æå†…å®¹ (UP)...":
            user_prompt = ""

        # ==========================================================
        # 2. åŠ¨æ€æŒ‡ä»¤æ„å»º (Dynamic Instruction Construction)
        # ==========================================================
        # æ ¸å¿ƒé€»è¾‘ï¼š
        # - SECTION 1 (ä¸»ä»»åŠ¡): ç”± system_command å†³å®šã€‚å¦‚æœç”¨æˆ·æ²¡å†™ï¼Œåˆ™ä½¿ç”¨å†…éƒ¨é»˜è®¤å€¼ã€‚
        # - SECTION 2/3 (é™„åŠ ä»»åŠ¡): ç”±å¼€å…³ (enable_tags/filename) å¼ºåˆ¶å†³å®šï¼Œç¡¬æ€§è¿½åŠ ã€‚
        
        # [Debug] æ‰“å°å¼€å…³çŠ¶æ€
        # print(f"\033[36m[UniversalAIChat] Tags Extraction: {enable_tags_extraction}, Filename Extraction: {enable_filename_extraction}\033[0m")

        # 2.1 ç¡®å®šåŸºç¡€æŒ‡ä»¤ (SECTION 1)
        # æ£€æŸ¥æ˜¯å¦ä¸ºé»˜è®¤ SC (ç©ºï¼Œæˆ–è€…ä½¿ç”¨äº†å·²çŸ¥çš„é»˜è®¤æ¨¡æ¿)
        
        # å®šä¹‰å·²çŸ¥çš„é»˜è®¤æ¨¡æ¿ (ç”¨äºæ™ºèƒ½åˆ‡æ¢)
        # 1. ä¸­æ–‡é»˜è®¤ (INPUT_TYPES ä¸­çš„é»˜è®¤å€¼)
        DEFAULT_CN_VISION = "ä½ æ˜¯ä¸€ä¸ªAIæç¤ºè¯å¤§å¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\nSECTION 1:\nè¯·ç”¨è¿è´¯çš„è‡ªç„¶è¯­è¨€è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€è¡¨æƒ…ã€å¤´é¥°ã€æœé¥°ã€åŠ¨ä½œã€åœºæ™¯å’Œæ°›å›´ã€‚ä¸è¦ä½¿ç”¨åˆ—è¡¨ï¼ˆä¸å°‘äº300ä¸ªå•è¯ï¼‰ã€‚\nSECTION 2:\nè¯·è¾“å‡ºæ ‡å‡† Danbooru é£æ ¼æ ‡ç­¾ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚èŒƒå›´ï¼š1.ä¸»ä½“ä¸æ•°é‡(å¦‚ 1girl, solo)ï¼›2.å¤–è²Œç‰¹å¾(ä¿ç•™é¢œè‰²/å½¢æ€ï¼Œå¦‚ long hair, blue eyes)ï¼›3.è¡£ç€é…é¥°(å¦‚ white dress, glasses)ï¼›4.åŠ¨ä½œå§¿æ€(å¦‚ sitting, hand on hip)ï¼›5.æ„å›¾è§†è§’(å¦‚ upper body, close-up, from side)ï¼›6.ç¯å¢ƒèƒŒæ™¯ã€‚ç¦æ­¢ï¼šä¸»è§‚è¯„ä»·è¯(beautiful, amazing)åŠæƒé‡è¯­æ³•ã€‚\nSECTION 3:\nç”¨èŒä¸šè§†è§’ä¸ºå†…å®¹å–ä¸€ä¸ªç®€çŸ­çš„è‹±æ–‡æ ‡é¢˜ï¼ˆç”±ä¸‰ä¸ªä»£è¡¨æ€§åè¯ç»„æˆï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰ï¼Œç”¨æ–¹æ‹¬å·æ‹¬èµ·æ¥ï¼Œä¾‹å¦‚ï¼š[woman bed lamp]ã€‚ä¸è¦åŒ…å«åç¼€æˆ–æ•°å­—ã€‚"
        
        # 2. è‹±æ–‡é»˜è®¤ (Vision)
        DEFAULT_EN_VISION = (
            "Describe the image in detail.\n"
            "SECTION 1:\n"
            "Provide a detailed, natural language description of the image content, including subject, action, scene, and atmosphere. (Min 300 words)."
        )
        
        # 3. è‹±æ–‡é»˜è®¤ (Text)
        DEFAULT_EN_TEXT = (
            "Refine the following text.\n"
            "SECTION 1:\n"
            "Provide a refined, detailed version of the input text."
        )

        sc_stripped = system_command.strip()
        
        # åˆ¤å®šå½“å‰ SC æ˜¯å¦ä¸ºæŸç§é»˜è®¤å€¼
        is_cn_vision_default = (sc_stripped == DEFAULT_CN_VISION.strip())
        is_en_vision_default = (sc_stripped == DEFAULT_EN_VISION.strip())
        is_en_text_default = (sc_stripped == DEFAULT_EN_TEXT.strip())
        is_empty = (not sc_stripped)

        # æ™ºèƒ½åˆ‡æ¢é€»è¾‘ï¼š
        # 1. å¦‚æœä¸ºç©º -> å¡«è¡¥é»˜è®¤å€¼ (Vision/Text å¯¹åº”)
        # 2. å¦‚æœæ˜¯ Vision ä»»åŠ¡ï¼Œä½† SC æ˜¯ Text é»˜è®¤å€¼ -> åˆ‡æ¢ä¸º Vision é»˜è®¤
        # 3. å¦‚æœæ˜¯ Text ä»»åŠ¡ï¼Œä½† SC æ˜¯ Vision é»˜è®¤å€¼ -> åˆ‡æ¢ä¸º Text é»˜è®¤
        # 4. å¦‚æœæ˜¯ Vision ä»»åŠ¡ï¼Œä¸” SC æ˜¯ Vision é»˜è®¤å€¼ (æ— è®ºæ˜¯ CN è¿˜æ˜¯ EN) -> ä¿æŒä¸å˜ (å°Šé‡ç”¨æˆ·é€‰æ‹©çš„è¯­è¨€)
        
        new_sc = None
        
        if is_empty:
             new_sc = DEFAULT_EN_VISION if is_vision_task else DEFAULT_EN_TEXT
             # print(f"\033[36m[UniversalAIChat] System Command is empty. Using default {'Vision' if is_vision_task else 'Text'} Prompt.\033[0m")
        
        elif is_vision_task:
             if is_en_text_default:
                 new_sc = DEFAULT_EN_VISION
                 # print(f"\033[36m[UniversalAIChat] Auto-switched from Text Default to Vision Default.\033[0m")
             # å¦‚æœæ˜¯ CN_VISION_DEFAULTï¼Œè™½ç„¶æ˜¯é»˜è®¤å€¼ï¼Œä½†é€‚ç”¨äº Visionï¼Œæ‰€ä»¥ä¿ç•™ï¼Œä¸å¼ºåˆ¶è½¬ EN
        
        else: # Text Task
             if is_cn_vision_default or is_en_vision_default:
                 new_sc = DEFAULT_EN_TEXT
                 # print(f"\033[36m[UniversalAIChat] Auto-switched from Vision Default to Text Default.\033[0m")

        if new_sc:
            system_command = new_sc

        # 2.2 æ„å»ºé™„åŠ æŒ‡ä»¤ (SECTION 2 & 3)
        extra_instructions = ""
        required_sections = ["SECTION 1"]
        
        if enable_tags_extraction:
            extra_instructions += (
                "\n\nSECTION 2:\n"
                "Extract Danbooru-style tags based on the generated description in SECTION 1. Comma-separated.\n"
                "Rule 1: MUST start with subject tags (e.g., 1girl, solo, man, 2boys).\n"
                "Rule 2: Followed by appearance, clothes, pose, background.\n"
                "Rule 3: No weights. No subjective words."
            )
            required_sections.append("SECTION 2")
            
        if enable_filename_extraction:
            extra_instructions += (
                "\n\nSECTION 3:\n"
                "Create a short title (3 words max, lower_case_with_underscores) for the generated description in SECTION 1. Output in brackets, e.g., [morning_coffee]."
            )
            required_sections.append("SECTION 3")

        # 2.3 æ„å»ºæ ¼å¼çº¦æŸ (Footer)
        # æç®€ç‰ˆ Footerï¼Œä»…åˆ—å‡ºæ¸…å•
        footer_instruction = ""
        if len(required_sections) > 1:
            req_str = ", ".join(required_sections)
            footer_instruction = (
                f"\n\nOUTPUT FORMAT REQUIRED:\n"
                f"You must output {req_str} in order.\n"
                f"Do not output anything else."
            )
        
        # ==========================================================
        # 3. æ„é€ æ¶ˆæ¯å†…å®¹ (Message Content Construction)
        # ==========================================================
        
        current_user_content = None
        display_up = ""

        if is_vision_task:
            # [Vision Mode]
            # ç»„åˆï¼š[Image] + [User Prompt (Hints)] + [System Command (Task)] + [Extra (Tags/File)] + [Footer]
            
            # æ³¨æ„ï¼šå¯¹äº Vision æ¨¡å‹ï¼Œé€šå¸¸å»ºè®®æŠŠ Task æ”¾åœ¨ Image ä¹‹å
            
            # å¦‚æœæ˜¯é»˜è®¤ SCï¼Œsystem_command å·²ç»æ˜¯å®Œæ•´çš„ Base Instruction
            # å¦‚æœæ˜¯è‡ªå®šä¹‰ SCï¼Œsystem_command æ˜¯ç”¨æˆ·çš„æŒ‡ä»¤
            
            # ç»„åˆ Text éƒ¨åˆ†
            # ç»“æ„ï¼š[SC/Base] + [User Prompt] + [Extra] + [Footer]
            
            final_text_parts = []
            
            # Part 1: System Command (Base Task)
            final_text_parts.append(system_command)
            
            # Part 2: User Prompt (Hints) - å¦‚æœæœ‰çš„è¯
            if user_prompt:
                final_text_parts.append(f"\n[User Hint/Input]: {user_prompt}")
            
            # Part 3: Extra Sections
            if extra_instructions:
                final_text_parts.append(extra_instructions)
            
            # Part 4: Footer
            if footer_instruction:
                final_text_parts.append(footer_instruction)
            
            final_vision_text = "\n\n".join(final_text_parts)
            display_up = f"[IMAGE]\n{final_vision_text}"
            
            # å›¾åƒå¤„ç†
            i = 255. * image[0].cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
            if img.mode != "RGB": img = img.convert("RGB")
            buffered = BytesIO()
            img.save(buffered, format="JPEG", quality=95)
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            current_user_content = [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}},
                {"type": "text", "text": final_vision_text}
            ]
            
            # Vision ä»»åŠ¡ä¸ä½¿ç”¨ç‹¬ç«‹çš„ System Message (åˆå¹¶åˆ° User Text)
            system_command_for_msg = "" 
            
        else:
            # [Text Mode]
            # ç»“æ„ï¼šSystem Message = system_command
            # User Message = user_prompt + [Extra] + [Footer]
            
            # System Message ä¿æŒä¸º system_command
            system_command_for_msg = system_command

            # å¦‚æœæ˜¯é»˜è®¤ SCï¼Œæˆ‘ä»¬å·²ç»æŠŠå®ƒæ”¹å†™æˆäº† "ä½ æ˜¯ä¸€ä¸ª...ä¸“å®¶...SECTION 1..."
            # å¦‚æœæ˜¯è‡ªå®šä¹‰ SCï¼Œä¿æŒåŸæ ·
            
            # å¤„ç† User Prompt ç©ºå€¼ fallback
            if not user_prompt.strip():
                if chat_mode == "Enhance_Prompt":
                    user_prompt = "Please proceed with the task."
                else:
                    user_prompt = "Hello."
            
            # æ„å»º User Message çš„åç¼€ (Extra + Footer)
            user_suffix_parts = []
            if extra_instructions:
                user_suffix_parts.append(extra_instructions)
            if footer_instruction:
                user_suffix_parts.append(footer_instruction)
            
            user_suffix = "\n\n".join(user_suffix_parts)
            
            final_user_text = f"{user_prompt}\n{user_suffix}" if user_suffix else user_prompt
            
            current_user_content = final_user_text
            
            # [Display Logic Improvement]
            # è®© Monitor æ˜¾ç¤ºå®Œæ•´ä¸Šä¸‹æ–‡ (åŒ…å« System Command)ï¼Œæ¶ˆé™¤ç”¨æˆ·å¯¹â€œæŒ‡ä»¤æ˜¯å¦ç”Ÿæ•ˆâ€çš„ç–‘è™‘
            if system_command_for_msg:
                 display_up = f"ğŸ›¡ï¸ [System Instruction]:\n{system_command_for_msg}\n\nğŸ‘¤ [User Input]:\n{final_user_text}"
            else:
                 display_up = final_user_text

        # 4. æ„é€ å®Œæ•´æ¶ˆæ¯é“¾ (Messages List)
        messages = []
        
        # System Message (ä»… Text Mode)
        if system_command_for_msg:
             messages.append({"role": "system", "content": system_command_for_msg})

        
        # Rule 2: Context æ³¨å…¥
        # [Universal Support - Modified]
        # ç”¨æˆ·çº æ­£ï¼šMonitor åªæ˜¯å­˜å‚¨ï¼Œé€šå¸¸ä¸è¿çº¿ç»™ Chatã€‚
        # åªæœ‰åœ¨ Debug æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·æ‰ä¼šæ‰‹åŠ¨è¿çº¿æˆ–å¤åˆ¶å†…å®¹ã€‚
        # ä½†å¦‚æœç”¨æˆ·åœ¨ Enhance_Prompt æ¨¡å¼ä¸‹ä¹Ÿè¿äº†çº¿å‘¢ï¼Ÿ
        # ç”¨æˆ·åŸè¯ï¼šâ€œä¸éœ€è¦ï¼Œé™¤é æˆ‘åœ¨debugæ¨¡å¼çš„æƒ…å†µä¸‹ï¼Œæˆ‘æ‰éœ€è¦æ‰‹åŠ¨å¤åˆ¶ï¼Œæˆ–è€…è¿çº¿ç»™chat.â€
        # è¿™æ„å‘³ç€ï¼šå¦‚æœè¿äº†çº¿ï¼Œæˆ‘ä»¬åº”è¯¥å°Šé‡è¿çº¿ã€‚
        # ä½†å¦‚æœ Context å¯¼è‡´äº†æˆªæ–­ï¼Œè¯´æ˜ Context å¤ªé•¿äº†ã€‚
        
        # æ—¢ç„¶ç”¨æˆ·è¯´â€œè¿™æ˜¯é”™çš„ï¼Œå¹¶æ²¡æœ‰â€ï¼Œé‚£è¯´æ˜åˆšæ‰çš„â€œç¬¬äºŒè½®æˆªæ–­â€å¹¶éå› ä¸º Contextï¼ˆå› ä¸ºç”¨æˆ·å¯èƒ½æ ¹æœ¬æ²¡è¿ Contextï¼‰ã€‚
        # å¦‚æœæ²¡è¿ Contextï¼Œä¸ºä»€ä¹ˆä¼šæˆªæ–­ï¼Ÿ
        # 1. ç¬¬ä¸€è½®ç”Ÿæˆçš„å¤ªé•¿ï¼Œå¯¼è‡´ System Command + User Prompt + Output > 2048ï¼Ÿ
        # 2. æˆ–è€…ç”¨æˆ·å…¶å®è¿äº† Context ä½†è‡ªå·±æ²¡æ„è¯†åˆ°ï¼Ÿ
        # 3. æˆ–è€…æ¨¡å‹è‡ªå·±åœ¨å‘ç–¯ï¼Ÿ
        
        # æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬å…ˆæŠŠ Context æ³¨å…¥é€»è¾‘æ”¹å›â€œå°Šé‡è¿çº¿â€ã€‚
        # åªè¦ context æœ‰å€¼ï¼Œå°±æ³¨å…¥ã€‚è¿™æ²¡é—®é¢˜ã€‚
        
        # å…³é”®æ˜¯ï¼Œç”¨æˆ·è¯´â€œç¬¬äºŒè½®è¾“å‡ºä¸å®Œæ•´â€ï¼Œå¦‚æœæ²¡è¿ Contextï¼Œé‚£ç¬¬äºŒè½®å’Œç¬¬ä¸€è½®åº”è¯¥æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼ˆå‡è®¾ Prompt æ²¡å˜ï¼‰ã€‚
        # å¦‚æœç¬¬äºŒè½®æ˜¯é’ˆå¯¹ç¬¬ä¸€è½®çš„æ¶¦è‰²ï¼ˆæ¯”å¦‚æŠŠç¬¬ä¸€è½®çš„è¾“å‡ºä½œä¸ºç¬¬äºŒè½®çš„è¾“å…¥ï¼‰ï¼Œé‚£ä¹ˆè¾“å…¥ç¡®å®å˜é•¿äº†ã€‚
        
        if context and context.strip():
            # ... (ä¿æŒæ³¨å…¥é€»è¾‘ä¸å˜ï¼Œå› ä¸ºåªæœ‰è¿äº†çº¿ context æ‰æœ‰å€¼)
            pass
            
            context_header = "\n\n## Historical Context (Reference Only):\n"
            
            if is_vision_task:
                 current_text = current_user_content[1]["text"]
                 new_text = f"{context_header}{context}\n\n{current_text}"
                 current_user_content[1]["text"] = new_text
            else:
                 messages.append({"role": "user", "content": f"{context_header}{context}"})

        
        # Rule 3: User Input (Text + Image or Text only)
        # [Critical Fix for VL Models]
        # å¯¹äºæŸäº› VL æ¨¡å‹ (å¦‚ Qwen-VL, Llava)ï¼Œå¦‚æœ content æ˜¯ list æ ¼å¼ï¼Œå¿…é¡»ç¡®ä¿æ ¼å¼å®Œå…¨ç¬¦åˆ llama-cpp-python çš„é¢„æœŸã€‚
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¶ˆæ¯ç»“æ„
        print(f"\033[36m[UniversalAIChat] Input Messages: {len(messages)} items\033[0m")
        if is_vision_task:
             print(f"\033[36m[UniversalAIChat] Vision Task Detected. Image Size: {len(img_str)} chars\033[0m")
             
        messages.append({"role": "user", "content": current_user_content})

        # æ¨ç†æ‰§è¡Œ
        try:
            # [Vision Mode Context Warning]
            # è§†è§‰ä»»åŠ¡é€šå¸¸éœ€è¦è¾ƒé•¿çš„ Context (å›¾ç‰‡ Token + ç”Ÿæˆå†…å®¹)
            if is_vision_task:
                 # è·å–å½“å‰ n_ctx
                 current_n_ctx = model.n_ctx() if hasattr(model, 'n_ctx') else 0
                 if current_n_ctx < 4096:
                     print(f"\033[31m[UniversalAIChat] CRITICAL WARNING: Vision task requires at least 4096 ctx. Current: {current_n_ctx}.\033[0m")
                     print(f"\033[33m[UniversalAIChat] Please increase 'n_ctx' in Loader node to avoid truncation or errors.\033[0m")

            # [Stop Token Handling]
            # å¼ºåˆ¶é”å®šåœæ­¢è¯ï¼Œé˜²æ­¢æ¨¡å‹æ— é™ç”Ÿæˆæˆ–åå‡ºç‰¹æ®Šæ ‡è®°
            # User Suggestion: Add <|im_start|> to stop tokens to prevent hallucinating new turns.
            stop_tokens = ["<|im_end|>", "<|endoftext|>", "<|im_start|>"]
            
            # [Temperature Guard]
            safe_temperature = min(max(temperature, 0.0), 2.0)
            if safe_temperature > 1.5:
                print(f"\033[33m[UniversalAIChat] Warning: High temperature ({safe_temperature}) detected. Output may be incoherent.\033[0m")

            # [Execution Logic Split]
            # User Request: å¼ºåˆ¶ ChatML æ ¼å¼ï¼Œä¸”åœ¨ Text æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ create_chat_completion (é¿å…é”™è¯¯æ¨¡æ¿)
            
            if is_vision_task:
                # [Vision Task]
                # å¿…é¡»ä½¿ç”¨ create_chat_completionï¼Œå› ä¸º image å¤„ç†é€»è¾‘å°è£…åœ¨ chat_handler ä¸­
                # æˆ‘ä»¬å°è¯•å¼ºåˆ¶ä¿®æ­£ chat_formatï¼Œä½†ä¸»è¦ä¾èµ– handler
                
                # å°è¯•ä¸´æ—¶è¦†ç›– format (å¦‚æœæ”¯æŒ)
                # model.chat_format = 'chatml' 
                
                output = model.create_chat_completion(
                    messages=messages, 
                    max_tokens=max_tokens, 
                    temperature=safe_temperature, 
                    repeat_penalty=repetition_penalty, 
                    seed=seed,
                    stop=stop_tokens
                )
                
                if not output or 'choices' not in output or not output['choices']:
                     raise ValueError("Empty response from model.")
                
                finish_reason = output['choices'][0].get('finish_reason', 'unknown')
                usage = output.get('usage', {})
                full_res = output['choices'][0]['message']['content']
                
            else:
                # [Text Task]
                # User Request (via AI Advice): 
                # 1. Abandon messages/create_chat_completion to avoid Llama-2 template errors.
                # 2. Manually construct ChatML string with System/User roles.
                # 3. Use create_completion (basic inference).
                
                prompt_parts = []
                
                # Part 1: System
                if system_command_for_msg:
                    prompt_parts.append(f"<|im_start|>system\n{system_command_for_msg}<|im_end|>\n")
                
                # Part 2: User
                # current_user_content includes User Prompt + Extra Instructions + Footer
                prompt_parts.append(f"<|im_start|>user\n{current_user_content}<|im_end|>\n")
                
                # Part 3: Assistant Start
                prompt_parts.append("<|im_start|>assistant\n")
                
                final_prompt = "".join(prompt_parts)
                
                print(f"\033[36m[UniversalAIChat] Manual ChatML Prompt Constructed ({len(final_prompt)} chars)\033[0m")
                # Debug: Print first 100 chars to verify format
                print(f"\033[36m[UniversalAIChat] Prompt Head: {final_prompt[:100].replace(chr(10), '\\n')}...\033[0m")
                
                # 3. è°ƒç”¨ create_completion (Raw)
                output = model.create_completion(
                    prompt=final_prompt,
                    max_tokens=max_tokens,
                    temperature=safe_temperature,
                    repeat_penalty=repetition_penalty,
                    seed=seed,
                    stop=stop_tokens
                )
                
                if not output or 'choices' not in output or not output['choices']:
                     raise ValueError("Empty response from model.")

                finish_reason = output['choices'][0].get('finish_reason', 'unknown')
                usage = output.get('usage', {})
                full_res = output['choices'][0]['text'] # create_completion è¿”å› 'text' å­—æ®µ

            
            print(f"\033[36m[UniversalAIChat] Usage: {usage}, Finish Reason: {finish_reason}\033[0m")
            
            if finish_reason == 'length':
                print(f"\033[31m[UniversalAIChat] WARNING: Output Truncated! Context Limit Reached.\033[0m")
                print(f"\033[33m[UniversalAIChat] Solution: Please increase 'n_ctx' in LoraHelper_Loader node (Current default is 2048, try 8192 or 16384).\033[0m")
                full_res += "\n\n[SYSTEM: Output Truncated due to Context Limit (n_ctx). Please increase it in Loader node.]"
            
            # [Post-Processing] æ¸…ç†å¯èƒ½æ®‹ç•™çš„ Token
            if full_res:
                 for token in ["[/INST]", "[INST]", "<|im_end|>", "<|endoftext|>", "<|im_start|>"]:
                     full_res = full_res.replace(token, "")
            
            # [Anti-Repetition Guard]
            # æ£€æµ‹å¹¶ç§»é™¤ System Command å¤è¯»
            # å¦‚æœ full_res ä»¥ system_command å¼€å¤´ï¼ˆå…è®¸å°‘é‡å·®å¼‚ï¼‰ï¼Œåˆ™ç§»é™¤
            if system_command and len(system_command) > 10:
                # ç®€å•çš„å‰ç¼€æ£€æŸ¥
                if full_res.strip().startswith(system_command.strip()[:20]):
                    print(f"\033[33m[UniversalAIChat] Warning: System Command repetition detected at start. Attempting to clean...\033[0m")
                    # å°è¯•æ‰¾åˆ° System Command çš„ç»“æŸä½ç½®
                    # è¿™é‡Œå‡è®¾ System Command æ˜¯å®Œæ•´çš„
                    if system_command.strip() in full_res:
                        full_res = full_res.replace(system_command.strip(), "", 1).strip()
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…ï¼Œå¯èƒ½æ˜¯å› ä¸º Tokenization å¯¼è‡´çš„å¾®å°å·®å¼‚
                        # å°è¯•ç§»é™¤å‰ N ä¸ªå­—ç¬¦ï¼Ÿé£é™©è¾ƒå¤§ã€‚
                        # å°è¯•åŒ¹é… SECTION 1 ä¹‹å‰çš„å†…å®¹
                        pass
            
            if not full_res:
                 # å°è¯•è·å– finish_reasonï¼Œçœ‹æ˜¯å¦æ˜¯å› ä¸º token é™åˆ¶æˆ–å…¶ä»–åŸå› æˆªæ–­
                 print(f"\033[33m[UniversalAIChat] Empty Content. Finish Reason: {finish_reason}\033[0m")
                 
        except Exception as e:
            error_msg = str(e)
            full_res = f"Error: {error_msg}"
            print(f"\033[31m[UniversalAIChat] Generation Error: {error_msg}\033[0m")
            
            # [Friendly Error Handler]
            # é’ˆå¯¹å¸¸è§çš„ "No KV slot available" é”™è¯¯ç»™å‡ºä¸­æ–‡å»ºè®®
            if "No KV slot available" in error_msg:
                 print(f"\033[31m[UniversalAIChat] ğŸ”´ é”™è¯¯è¯Šæ–­: ä¸Šä¸‹æ–‡é•¿åº¦ (n_ctx) ä¸è¶³ï¼\033[0m")
                 print(f"\033[33m[UniversalAIChat] ğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¯·åœ¨ [Qwen3_GGUF_loader] èŠ‚ç‚¹ä¸­ï¼Œå°† 'n_ctx' çš„å€¼è°ƒå¤§ã€‚\033[0m")
                 print(f"\033[33m[UniversalAIChat]    - å½“å‰å¯èƒ½è®¾ç½®è¿‡å°ï¼Œå»ºè®®å°è¯• 8192, 16384 æˆ– 32768ã€‚\033[0m")
                 print(f"\033[33m[UniversalAIChat]    - è§†è§‰ä»»åŠ¡(Vision)é€šå¸¸éœ€è¦æ›´å¤§çš„ä¸Šä¸‹æ–‡ç©ºé—´ã€‚\033[0m")
                 
                 full_res += "\n\n[SYSTEM ERROR]: Context Window Full (n_ctx too small). Please increase 'n_ctx' in the Loader node."

        # 4. æ™ºèƒ½æˆªå– (Smart Truncation)
        # ç”¨æˆ·è¦æ±‚ gen_text åªåŒ…å« SECTION 1, 2, 3ã€‚
        # æ— è®ºæ˜¯å¦æœ‰ <think> æ ‡ç­¾ï¼Œå¦‚æœæ£€æµ‹åˆ° "SECTION 1:"ï¼Œåˆ™ä¸¢å¼ƒå…¶ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ã€‚
        
        # Step A: æ ‡å‡† think æ ‡ç­¾æ¸…ç† (é’ˆå¯¹é—­åˆçš„æ ‡ç­¾)
        clean_text = re.sub(r'<think>.*?</think>', '', full_res, flags=re.DOTALL).strip()
        
        # [å¼ºåŒ–æ¸…ç†] å¦‚æœæ¸…ç†åä»ä»¥ <think> å¼€å¤´ (è¯´æ˜æ²¡æœ‰é—­åˆ)ï¼Œå°è¯•æš´åŠ›ç§»é™¤ç›´åˆ°çœŸæ­£çš„æ­£æ–‡
        # ç­–ç•¥ï¼šå¦‚æœæ‰¾ä¸åˆ° </think>ï¼Œä½†èƒ½æ‰¾åˆ° SECTION 1ï¼Œåˆ™ä¸¢å¼ƒ SECTION 1 ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
        if clean_text.startswith('<think>'):
             # å°è¯•å¯»æ‰¾ </think> çš„å˜ä½“
             end_think = clean_text.find('</think>')
             if end_think != -1:
                 clean_text = clean_text[end_think+8:].strip()
             else:
                 # æ²¡æ‰¾åˆ°é—­åˆæ ‡ç­¾ï¼Œä¾èµ–ä¸‹é¢çš„ SECTION é”šç‚¹æˆªå–
                 pass

        # Step B: æ™ºèƒ½é”šç‚¹æˆªå– (Smart Anchor Truncation)
        # ç­–ç•¥å‡çº§ï¼šä¼˜å…ˆåŒ¹é…â€œè¡Œé¦–â€çš„ SECTION 1ï¼Œä»¥é¿å…åŒ¹é…åˆ°æ–‡ä¸­å¼•ç”¨çš„ SECTION 1ã€‚
        # åŒæ—¶æ”¾å¼ƒ rpartition (ä»åå¾€å‰æ‰¾)ï¼Œæ”¹å›ä»å‰å¾€åæ‰¾ï¼Œé˜²æ­¢å› æ–‡æœ«æ€»ç»“åŒ…å« SECTION 1 è€Œå¯¼è‡´æ•´ä¸ªæ­£æ–‡è¢«æˆªæ–­ã€‚
        
        # [Refined Logic] æ’é™¤ System Command ä¸­çš„ "SECTION 1: è‡ªç„¶è¯­è¨€æè¿°"
        # æˆ‘ä»¬å¯ä»¥æŸ¥æ‰¾ "SECTION 1:" ä¸”åé¢ä¸ç´§è·Ÿ " è‡ªç„¶è¯­è¨€æè¿°" çš„æƒ…å†µ
        # æˆ–è€…æ›´é€šç”¨åœ°ï¼ŒæŸ¥æ‰¾ SECTION 1: åé¢æœ‰æ¢è¡Œæˆ–è€…éæŒ‡ä»¤æ–‡æœ¬
        
        target_anchor_pattern = r'(?:^|\n)SECTION 1:(?!\s*è‡ªç„¶è¯­è¨€æè¿°)'
        match = re.search(target_anchor_pattern, clean_text)
        
        if match:
            # ä»åŒ¹é…åˆ°çš„ä½ç½®å¼€å§‹æˆªå–
            start_index = match.start()
            # å¦‚æœåŒ¹é…åˆ°çš„æ˜¯ \nSECTION 1:ï¼Œstart_index ä¼šåŒ…å« \nï¼Œæˆ‘ä»¬éœ€è¦ä¿ç•™ SECTION 1:
            # match.group() æ˜¯ "\nSECTION 1:" æˆ– "SECTION 1:"
            # æˆ‘ä»¬ç›´æ¥ä» match.start() + (1 if match.group().startswith('\n') else 0) å¼€å§‹?
            # ä¸ï¼Œç›´æ¥å– match.start() ä¹‹åçš„å†…å®¹å³å¯ï¼Œä¿ç•™ \n ä¹Ÿæ²¡å…³ç³»ï¼Œåæ­£åé¢æœ‰ strip()
            
            # ç²¾ç¡®å¤„ç†ï¼šæ‰¾åˆ° "SECTION 1:" çš„èµ·å§‹ä½ç½®
            real_start = clean_text.find("SECTION 1:", start_index)
            clean_text = clean_text[real_start:]
        else:
            # Fallback: å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•å®½å®¹åŒ¹é… (ä¸å¸¦æ¢è¡Œç¬¦é™åˆ¶)
            # ä½†ä»ç„¶ä½¿ç”¨ find (ä»å·¦å¾€å³)ï¼Œä»¥é˜²è¯¯åˆ 
            first_anchor = clean_text.find("SECTION 1:")
            if first_anchor != -1:
                clean_text = clean_text[first_anchor:]
            else:
                # å®¹é”™ï¼šå¦‚æœæ²¡æ‰¾åˆ° SECTION 1ï¼Œä½†æ‰¾åˆ°äº† SECTION 2 (æç½•è§æƒ…å†µ)
                # åŒæ ·ä½¿ç”¨ find
                pass
            target_anchor_2 = "SECTION 2:"
            if target_anchor_2 in clean_text:
                start_pos = clean_text.find(target_anchor_2)
                clean_text = clean_text[start_pos:]
            
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¯´æ˜å¯èƒ½ä¸æ˜¯æ ‡å‡†æ ¼å¼è¾“å‡ºï¼Œæˆ–è€…ç”¨æˆ·ç”¨äº†è‡ªå®šä¹‰ Promptã€‚
            # æ­¤æ—¶å†å°è¯•å¤„ç† "æœªé—­åˆçš„ <think>" (å³åªæœ‰ <think> æ²¡æœ‰ </think>)
            # ç­–ç•¥ï¼šå¦‚æœå¼€å¤´æ˜¯ <think>ï¼Œä¸”æ‰¾ä¸åˆ° </think>ï¼Œè¿™é€šå¸¸æ„å‘³ç€æ•´ä¸ªè¾“å‡ºéƒ½æ˜¯æ€è€ƒè¿‡ç¨‹æˆ–è€…è¢«æˆªæ–­äº†ã€‚
            # ä½†æ—¢ç„¶æ²¡æ‰¾åˆ° SECTIONï¼Œæˆ‘ä»¬æœ€å¥½è¿˜æ˜¯ä¿ç•™å®ƒï¼Œæˆ–è€…æ˜¯ç»™ä¸ªæç¤ºï¼Ÿ
            # [Aggressive Fix] å¦‚æœè¿˜æ˜¯ä»¥ <think> å¼€å¤´ï¼Œè¯´æ˜æ•´ä¸ªå›å¤éƒ½æ˜¯æ€è€ƒè¿‡ç¨‹ï¼Œæˆ–è€…æ­£æ–‡è¢«åäº†
            if clean_text.startswith('<think>'):
                # å°è¯•åªä¿ç•™æœ€åä¸€éƒ¨åˆ†æ–‡æœ¬ï¼ˆé£é™©è¾ƒå¤§ï¼‰ï¼Œæˆ–è€…æç¤ºé”™è¯¯
                # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™åŸæ ·ï¼Œä½†åœ¨ Monitor é‡Œå¯èƒ½ä¼šæ¯”è¾ƒéš¾çœ‹
                pass


        # [Critical Correction] Monitor æ•°æ®æµ
        # ç”¨æˆ·çº æ­£ï¼šChat åº”è¯¥æŠŠâ€œæ–‡æœ¬åŸæ ·ä¸åŠ¨â€ç»™ Monitorï¼Œè¿ think è¿‡ç¨‹éƒ½è¦ä¿ç•™ã€‚
        # Monitor è´Ÿè´£æ•´ç†åŸå§‹å¯¹è¯å†å²ã€‚
        # è€Œ Description/Tags/Filename ç«¯å£è¾“å‡ºçš„æ˜¯ç»è¿‡æ™ºèƒ½æˆªå–å’Œåˆ†å‰²çš„å†…å®¹ã€‚
        
        # å› æ­¤ï¼Œchat_history ä½¿ç”¨ full_res (ä¿ç•™ <think> æ ‡ç­¾å’Œå®Œæ•´å†…å®¹)
        raw_clean_text = full_res
            
        chat_history = f"User: {display_up}\nAI: {raw_clean_text}"

        if release_vram:
            gc.collect()
            torch.cuda.empty_cache()
            
        # 5. å†…ç½® Splitter é€»è¾‘ (Built-in Splitter)
        # å°† clean_text (å·²æˆªå– SECTION 1 ä¹‹åçš„å†…å®¹) åˆ‡åˆ†ä¸º description, tags, filename
        # é»˜è®¤å€¼
        out_desc = clean_text
        out_tags = ""
        out_filename = ""
        
        # å°è¯•è§£æ SECTION æ ¼å¼
        # æ ¼å¼é¢„æœŸ:
        # SECTION 1: xxx
        # SECTION 2: xxx
        # SECTION 3: xxx
        
        # [User Correction] æˆªå–åŠ¨ä½œè¦åœ¨ context ä¹‹å‰ï¼Ÿ
        # ç”¨æˆ·è¯´ï¼šâ€œæˆ‘ä»¬åšæˆªå–çš„æ—¶å€™ï¼Œä¸æ˜¯ä»è¿™ä¸ªäº”è½®çš„æ–‡å­—é‡Œæˆªå–ï¼Œæ˜¯ä»æ¯ä¸€æ¬¡è¾“å‡ºçš„æ–‡æœ¬é‡Œæˆªå–ï¼Œæˆªå–åŠ¨ä½œè¦åœ¨contextè¿™ä¸ªåŠ¨ä½œä¹‹å‰ã€‚â€
        # ç†è§£ï¼šç”¨æˆ·å¯èƒ½æ˜¯åœ¨çº æ­£æˆ‘ä¹‹å‰çš„å›å¤ï¼ˆæˆ‘ä¹‹å‰è¯´æŠŠ Monitor çš„å†å²å…¨å¡ç»™ Chatï¼‰ã€‚
        # ç°åœ¨çš„ä»£ç é€»è¾‘æ­£æ˜¯å¦‚æ­¤ï¼š
        # 1. full_res (Model Output) -> 2. clean_text (Smart Truncation/æˆªå–) -> 3. Splitter (è§£æ) -> 4. Return
        # è€Œ chat_history ä¹Ÿæ˜¯åŸºäº clean_text ç”Ÿæˆçš„ã€‚
        # æ‰€ä»¥ç›®å‰çš„æˆªå–é€»è¾‘æ˜¯ä½œç”¨äºâ€œå•æ¬¡è¾“å‡ºâ€çš„ï¼Œç¬¦åˆç”¨æˆ·è¦æ±‚ã€‚

        
        # æŸ¥æ‰¾å„ä¸ª Section çš„ä½ç½®
        # æ³¨æ„ï¼šç”±äºä¹‹å‰åšè¿‡â€œä»å³å‘å·¦æŸ¥æ‰¾ SECTION 1â€ï¼Œæ‰€ä»¥ clean_text ç†è®ºä¸Šæ˜¯ä» SECTION 1 å¼€å§‹çš„
        
        # å¦‚æœ clean_text ä¸åŒ…å« SECTION 1 å­—æ ·ï¼Œè¯´æ˜å¯èƒ½æ¨¡å‹æ²¡æŒ‰æ ¼å¼è¾“å‡ºï¼Œæˆ–è€…å·²ç»è¢«æˆªæ–­äº†
        # æˆ‘ä»¬ç”¨æ›´é€šç”¨çš„æ­£åˆ™æ¥æå–
        
        # æå– SECTION 1 (Description)
        # é€»è¾‘å‡çº§ï¼šæ— è®ºæ˜¯å¦æœ‰ "SECTION 1:" æ ‡ç­¾ï¼Œåªè¦æ˜¯åœ¨ SECTION 2/3 ä¹‹å‰çš„å†…å®¹ï¼Œéƒ½ç®—ä½œ Description
        # å…ˆå°è¯•æ ‡å‡†åŒ¹é… (æ³¨æ„ï¼šå¢åŠ äº†å¯¹æ¢è¡Œçš„å®¹é”™ï¼Œä¸”å…è®¸å†’å·ä¸¢å¤±)
        # æ­£åˆ™å«ä¹‰ï¼šæŸ¥æ‰¾ SECTION 1(å¯é€‰å†’å·) åé¢ï¼Œç›´åˆ°é‡åˆ° SECTION 2 æˆ– SECTION 3 æˆ–ç»“æŸ
        
        match_s1 = re.search(r'SECTION 1[:ï¼š]?\s*(.*?)(?=\n\s*SECTION 2|\n\s*SECTION 3|SECTION 2|SECTION 3|$)', clean_text, re.DOTALL | re.IGNORECASE)
        if match_s1:
            out_desc = match_s1.group(1).strip()
        else:
            # Fallback: å¦‚æœæ²¡æ‰¾åˆ° SECTION 1 æ ‡ç­¾ï¼Œå°è¯•æˆªå–å¼€å¤´åˆ°ç¬¬ä¸€ä¸ªå…¶ä»– SECTION çš„ä½ç½®
            # æ‰¾åˆ°æœ€æ—©å‡ºç°çš„ SECTION 2 æˆ– SECTION 3
            end_pos = len(clean_text)
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ­£åˆ™æŸ¥æ‰¾ SECTION 2/3ï¼Œå…è®¸å†’å·ä¸¢å¤±
            match_s2_start = re.search(r'(?:\n|^)\s*SECTION 2', clean_text, re.IGNORECASE)
            if match_s2_start:
                end_pos = min(end_pos, match_s2_start.start())
                
            match_s3_start = re.search(r'(?:\n|^)\s*SECTION 3', clean_text, re.IGNORECASE)
            if match_s3_start:
                end_pos = min(end_pos, match_s3_start.start())
            
            # æˆªå–
            candidate_desc = clean_text[:end_pos].strip()
            if candidate_desc:
                out_desc = candidate_desc
        
        # æå– SECTION 2 (Tags)
        # [Fix] å¢å¼ºå¯¹å†’å·çš„å®¹é”™ï¼Œæœ‰äº›æ¨¡å‹å¯èƒ½æ¼å†™å†’å·ï¼Œæˆ–è€…å†™æˆä¸­æ–‡å†’å·
        match_s2 = re.search(r'SECTION 2[:ï¼š]?\s*(.*?)(?=\nSECTION 3:|SECTION 3:|$)', clean_text, re.DOTALL | re.IGNORECASE)
        if match_s2:
            raw_tags = match_s2.group(1).strip()
            # æ¸…ç† tags: ç§»é™¤å¯èƒ½çš„ markdown åˆ—è¡¨ç¬¦ï¼Œç»Ÿä¸€é€—å·
            raw_tags = raw_tags.replace('\n', ',').replace('ã€', ',')
            # ç®€å•çš„å»é‡å’Œæ¸…ç†
            tags_list = [t.strip() for t in raw_tags.split(',') if t.strip()]
            out_tags = ", ".join(tags_list)
            
        # æå– SECTION 3 (Filename)
        match_s3 = re.search(r'SECTION 3[:ï¼š]?\s*(.*?)(?=$)', clean_text, re.DOTALL | re.IGNORECASE)
        if match_s3:
            raw_fn = match_s3.group(1).strip()
            # å°è¯•æå–æ–¹æ‹¬å·å†…çš„å†…å®¹
            match_bracket = re.search(r'\[(.*?)\]', raw_fn)
            if match_bracket:
                out_filename = match_bracket.group(1).strip()
            else:
                out_filename = raw_fn
                
        # è¿”å›åˆ‡åˆ†åçš„ç»“æœ
        return (out_desc, out_tags, out_filename, chat_history)

# 4. å†å²ç›‘æ§èŠ‚ç‚¹ (æµæ°´çº¿æ’åº)
# ==========================================================
# PROJECT: LoraHelper_Monitor (History Viewer)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. chat_history (Raw Text Input)
#
# LOGIC DEFINITION:
#   - Maintains a rolling buffer of last 5 chat interactions
#   - Output 1: context (Raw text for LLM)
#   - UI Display: Formatted cards (Old -> New)
# ==========================================================
class LH_History_Monitor:
    def __init__(self):
        self.history = []

    @classmethod
    def INPUT_TYPES(s):
        return { "required": { "chat_history": ("STRING", {"forceInput": True}) } }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    OUTPUT_NODE = True
    FUNCTION = "update"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def update(self, chat_history):
        # 1. è§£æè¾“å…¥ (æ”¯æŒ JSON æˆ– çº¯æ–‡æœ¬)
        import json
        user_msg = ""
        ai_msg = ""
        
        # å°è¯•è§£æç‰¹å®šæ ¼å¼ "User: ... \nAI: ..."
        if isinstance(chat_history, str) and chat_history.startswith("User:"):
             # ä½¿ç”¨ split åˆ†å‰²ï¼Œæ³¨æ„åªåˆ†å‰²ç¬¬ä¸€ä¸ª "\nAI: "
             parts = chat_history.split("\nAI: ", 1)
             if len(parts) == 2:
                 user_msg = parts[0][5:].strip() # å»æ‰ "User: "
                 ai_msg = parts[1].strip()
             else:
                 user_msg = "Raw Input"
                 ai_msg = str(chat_history)
        else:
            try:
                data = json.loads(chat_history)
                if isinstance(data, dict):
                    user_msg = data.get("user", "")
                    ai_msg = data.get("ai", "")
                else:
                    user_msg = "Raw Input"
                    ai_msg = str(chat_history)
            except:
                 user_msg = "Raw Input"
                 ai_msg = str(chat_history)
        
        # 2. æ›´æ–°å†å² (å»é‡)
        # æ„é€ ä¸€ä¸ªç»“æ„åŒ–å¯¹è±¡å­˜å‚¨
        new_entry = {"user": user_msg, "ai": ai_msg}
        
        # ç®€å•å»é‡ï¼šæ£€æŸ¥ä¸Šä¸€æ¡æ˜¯å¦å®Œå…¨ä¸€è‡´
        if self.history:
            last = self.history[-1]
            if last["user"] == user_msg and last["ai"] == ai_msg:
                pass # é‡å¤ï¼Œå¿½ç•¥
            else:
                self.history.append(new_entry)
        else:
            self.history.append(new_entry)
            
        # ä¿æŒ 5 è½®
        if len(self.history) > 5:
            self.history.pop(0)

        # 3. æ„é€  Context (ç”¨äºå›ä¼ ç»™ Chat)
        # æ ¼å¼ï¼šRound X User: ... \n Round X AI: ...
        context_parts = []
        for i, h in enumerate(self.history):
            context_parts.append(f"Round {i+1} User: {h['user']}")
            context_parts.append(f"Round {i+1} AI: {h['ai']}")
        context = "\n\n".join(context_parts)

        # 4. æ„é€  UI æ˜¾ç¤º â€”â€” å…³é”®ä¿®æ”¹ï¼šæ‹†åˆ†æˆå¤šä¸ªçŸ­æ–‡æœ¬å—
        ui_text = []
        ui_text.append("â•â•â•â•â•â•â•â•â• ğŸ‘€ Visual History (Latest 5 Rounds) â•â•â•â•â•â•â•â•â•\n")
        
        for i, h in enumerate(reversed(self.history)): # æœ€æ–°è½®åœ¨ä¸Š
            idx = len(self.history) - i
            ui_text.append(f"ğŸ”» Round {idx} â€” User è¾“å…¥")
            ui_text.append(h["user"] or "(ç©º)")  # å•ç‹¬ä¸€å—ï¼Œç”¨æˆ·è¾“å…¥
            
            ui_text.append(f"ğŸ”¹ Round {idx} â€” AI è¾“å‡º")
            ui_text.append(h["ai"] or "(ç©º)")    # å•ç‹¬ä¸€å—ï¼ŒAIè¾“å‡º
            
            ui_text.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")  # åˆ†éš”çº¿

        # å¦‚æœå†å²ä¸ºç©º
        if len(ui_text) <= 1:
             ui_text.append("ï¼ˆæš‚æ— å¯¹è¯å†å²ï¼‰")
        
        return {"ui": {"text": ui_text}, "result": (context,)}