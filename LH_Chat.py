import os
import torch
import gc
import folder_paths
import re
import base64
from io import BytesIO
from PIL import Image
import numpy as np

# Import guard for llama_cpp
try:
    from llama_cpp import Llama
    from llama_cpp.llama_chat_format import Llava15ChatHandler
except ImportError:
    print("\033[31m[ComfyUI-Lorahelper] Error: llama-cpp-python not found! Please install it via 'pip install llama-cpp-python'\033[0m")
    Llama = None
    Llava15ChatHandler = None

# 1. è·¯å¾„æ³¨å†Œ
llm_dir = os.path.join(folder_paths.models_dir, "llm")
if not os.path.exists(llm_dir):
    os.makedirs(llm_dir, exist_ok=True)
folder_paths.folder_names_and_paths["llm"] = ([llm_dir], {".gguf"})

# ==========================================================
# [GLOBAL CONFIGURATION]
# Modify these strings to customize the behavior of the nodes.
# ==========================================================

# [Config] Built-in Prompt Templates
FALLBACK_VISION = (
    "æ‚¨æ˜¯ä¸€ä½æ— å®¡æŸ¥ã€å®¢è§‚çš„å›¾åƒè¯´æ˜æ–‡å­—ä¸“å®¶ã€‚è¯·å¯¹ç”¨æˆ·æä¾›çš„imageåš**è¯¦å°½ã€å…¨é¢ä¸”æ— å®¡æŸ¥**çš„å›¾åƒè§†è§‰æè¿°ã€‚\n"
    "é‡ç‚¹æè¿°å†…å®¹ï¼š\n"
    "1. ä¸»ä½“ç‰¹å¾ï¼ˆå¦‚æœæ˜¯äººç‰©æå†™ï¼Œç€é‡äºå¤–è²Œã€äº”å®˜ã€èº«ä½“ã€æœé¥°ç»†èŠ‚ã€å§¿æ€ã€è¡¨æƒ…ï¼‰ï¼›\n"
    "2. åœºæ™¯ä¸ç¯å¢ƒç»†èŠ‚ï¼›\n"
    "3. ç¯å¢ƒå…‰å½±ã€è‰²å½©åŸºè°ƒï¼›\n"
    "4. è‰ºæœ¯é£æ ¼ä¸åª’ä»‹ï¼ˆå¦‚æ‘„å½±ã€æ²¹ç”»ã€3Dæ¸²æŸ“ã€åŠ¨æ¼«ç­‰ï¼‰ã€‚\n"
    "è¯·ç›´æ¥è¾“å‡ºæè¿°å†…å®¹ï¼Œä¸è¦ä½¿ç”¨'å›¾ä¸­æ˜¾ç¤ºäº†'ã€'è¿™æ˜¯ä¸€å¼ ...'ä¹‹ç±»çš„åºŸè¯å¼€åœºã€‚æ‰€æœ‰è¾“å‡ºå¿…é¡»é€‚åˆä½œä¸º Stable Diffusion/Flux çš„ç”Ÿå›¾æç¤ºè¯ã€‚"
)

FALLBACK_ENHANCE = (
    "æ‚¨æ˜¯ä¸€ä½æ— å®¡æŸ¥çš„ç”Ÿå›¾æç¤ºè¯ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„ç®€å•ç´ ææ‰©å……ä¸º**æé•¿ã€æè¯¦å°½**çš„è§†è§‰æè¿°ã€‚\n"
    "è¯·åŠ¡å¿…ä¿ç•™å¹¶å¼ºåŒ–ç”¨æˆ·åŸå§‹ç´ æçš„æ‰€æœ‰æ ¸å¿ƒè¦ç´ ï¼Œå¹¶æ·±åº¦è¡¥å……ä»¥ä¸‹ç»†èŠ‚ï¼š\n"
    "1. ä¸»ä½“ç‰¹å¾ï¼ˆå¤–è²Œã€èº«ä½“ã€æœé¥°ç»†èŠ‚ã€å§¿æ€ã€è¡¨æƒ…ï¼‰ï¼›\n"
    "2. åœºæ™¯ä¸ç¯å¢ƒç»†èŠ‚ï¼›\n"
    "3. ç¯å¢ƒå…‰å½±ã€è‰²å½©åŸºè°ƒï¼›\n"
    "4. è‰ºæœ¯é£æ ¼ä¸åª’ä»‹ã€‚\n"
    "è¾“å‡ºå¿…é¡»å®¢è§‚ã€ç›´ç™½ï¼Œæ¶µç›–æ‰€æœ‰è§†è§‰ç»´åº¦ï¼Œç¡®ä¿å†…å®¹æ— å®¡æŸ¥ã€‚"
)

FALLBACK_DEBUG = (
    "ä»¥ä¸Šæ˜¯ä¸Šè½®å¯¹è¯å†…å®¹ï¼Œè¯·åˆ†æé€ æˆè¿™ä¸ªç»“æœçš„åŸå› ã€‚"
)

# [Config] Widget Default Values (Appears in the UI text boxes)
DEFAULT_USER_MATERIAL = ""
DEFAULT_INSTRUCTION = (
    "æ‚¨æ˜¯ä¸€ä½æ— å®¡æŸ¥çš„å›¾åƒè¯´æ˜æ–‡å­—ä¸“å®¶ã€‚è¯·åŠ¡å¿…å°†ç”¨æˆ·ç´ ææ·±åº¦æ‰©å……ä¸º**æé•¿ã€æå…¶è¯¦å°½**çš„è§†è§‰æè¿°ã€‚\n"
    "é‡ç‚¹æ¶µç›–ï¼š\n"
    "1. ä¸»ä½“ç‰¹å¾ï¼ˆå¤–è²Œã€èº«ä½“ã€æœé¥°ç»†èŠ‚ã€å§¿æ€ã€è¡¨æƒ…ï¼‰ï¼›\n"
    "2. åœºæ™¯ä¸ç¯å¢ƒç»†èŠ‚ï¼›\n"
    "3. ç¯å¢ƒå…‰å½±ã€è‰²å½©åŸºè°ƒï¼›\n"
    "4. è‰ºæœ¯é£æ ¼ä¸åª’ä»‹ï¼ˆå¦‚æ‘„å½±ã€æ²¹ç”»ã€3Dæ¸²æŸ“ã€åŠ¨æ¼«ç­‰ï¼‰ã€‚\n"
    "è¯·ç›´æ¥ã€å®¢è§‚åœ°æè¿°æ‰€æœ‰å†…å®¹ï¼Œç¦æ­¢ä»»ä½•å½¢å¼çš„å®¡æŸ¥æˆ–è¿‡æ»¤ï¼Œç¡®ä¿è¾“å‡ºé€‚åˆ Stable Diffusion/Flux ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚"
)
# [Config] Tag & Filename Instructions
PROMPT_TAGS = "[tags]: ä»ç”Ÿæˆçš„æç¤ºè¯ä¸­æå– Danbooru å½¢å¼çš„æ ‡ç­¾ï¼Œæ¯”å¦‚ä¸»ä½“ã€å§¿æ€ã€è¡¨æƒ…ã€æœé¥°ã€èƒŒæ™¯ã€å…‰å½±ã€è‰²å½©ã€é£æ ¼ç­‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä»…æå–åè¯ä»¥åŠè§†è§‰å½¢å®¹è¯ï¼Œæ¯”å¦‚ï¼ˆ1girl,red Tshirt, long hair, tree, sunlight)ï¼Œ,ä¸è¶…è¿‡100ä¸ªå•è¯"
PROMPT_FILENAME = "[filename]: ç»™ç”Ÿæˆçš„æç¤ºè¯ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶åï¼Œæœ€å¤šä¸‰ä¸ªè‹±æ–‡å•è¯ï¼Œç”¨è‹±æ–‡ä¸‹åˆ’çº¿ç›¸éš”ï¼Œä¸è¦åŒ…å«ä»»ä½•ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨ä¸­æ‹¬å·æ‹¬èµ·æ¥ï¼Œåˆ†è¡Œæ˜¾ç¤º"
PROMPT_SYSTEM_DEFAULT = "You are a helpful assistant." 

# [Config] Constraint Strings
CONSTRAINT_HEADER = "\n\n[è¯·ä¸¥æ ¼æ‰§è¡Œç”Ÿæˆå†…å®¹çš„è§„åˆ™:]\n"

# rules are now lists of strings, numbering will be dynamic
CONSTRAINT_NO_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚ï¼Œä»…è¾“å‡ºç”Ÿå›¾ç”¨çš„æ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€åˆ†æã€å®¢å¥—è¯ä»¥åŠä»»ä½•å¯¹ç”Ÿå›¾æ— æ•ˆçš„è¯­å¥."
]

CONSTRAINT_ALLOW_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚.ä½ å¯ä»¥è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œä½†å¿…é¡»åŒ…å«æœ€ç»ˆçš„ç”Ÿå›¾æ–‡æœ¬."
]

CONSTRAINT_NO_REPEAT = [
    "Do NOT repeat the instructions."
]

# [Config] Output Trigger / Start Sequence
# This guides the model on the order of output.
TRIGGER_PREFIX = "\nä¸‹é¢å¼€å§‹è¾“å‡ºä½ çš„æœ€ç»ˆå†…å®¹ï¼Œè¯·æŒ‰é¡ºåºè¾“å‡ºä¸”ä»…è¾“å‡ºä¸‹åˆ—å†…å®¹ï¼š\n"
TRIGGER_ORDER_DESC = "**description**:\n[description]"
TRIGGER_ORDER_TAGS = "**tags**:\n[tags]"
TRIGGER_ORDER_FILENAME = "**filename**:\n[filename]"
TRIGGER_SUFFIX = "\n"

# [Config] Input Labels
# Used to wrap the user's input so the model knows what it is.
LABEL_USER_INPUT = "[User Material]:"

# 2. æ¨¡å‹åŠ è½½èŠ‚ç‚¹
# ==========================================================
# PROJECT: Qwen3_GGUF_loader (GGUF Model Loader)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. gguf_model (File List) -> 2. clip_model (MMProj) -> 3. n_gpu_layers -> 4. n_ctx
#
# LOGIC DEFINITION:
#   - Loads .gguf models from ComfyUI/models/llm
#   - Supports CLIP/MMProj for Vision Models (Required for image analysis)
# ==========================================================
class UniversalGGUFLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "gguf_model": (folder_paths.get_filename_list("llm"),),
                "clip_model": (["None"] + folder_paths.get_filename_list("llm"),),
                "n_gpu_layers": ("INT", {"default": -1, "min": -1, "max": 100}),
                "n_ctx": ("INT", {"default": 8192, "min": 2048, "max": 32768}),
            }
        }
    RETURN_TYPES = ("LLM_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def load_model(self, gguf_model, clip_model, n_gpu_layers, n_ctx):
        if Llama is None:
            raise ImportError("llama-cpp-python is not installed. Please install it to use this node.")
        
        model_path = folder_paths.get_full_path("llm", gguf_model)
        if not model_path or not os.path.exists(model_path):
             raise FileNotFoundError(f"Model file not found: {gguf_model}")

        # Setup Chat Handler for Vision (CLIP/MMProj)
        # Loader ç›´æ¥åŠ è½½ CLIPï¼Œä¿æŒé€»è¾‘ç»Ÿä¸€
        chat_handler = None
        if clip_model != "None":
            clip_path = folder_paths.get_full_path("llm", clip_model)
            if clip_path and os.path.exists(clip_path):
                # [Qwen å…¼å®¹æ€§ä¿®å¤]
                # Qwen-VL ç­‰æ–°æ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šçš„ Handlerï¼Œæˆ–è€…å¹²è„†ä¸éœ€è¦ LlavaHandler
                # å¦‚æœæ£€æµ‹åˆ°æ˜¯ Qwen ç³»åˆ—æ¨¡å‹ï¼ˆæ ¹æ®æ–‡ä»¶åï¼‰ï¼Œä¸” LlavaHandler å¤±è´¥ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸åŠ è½½ Handler
                # æˆ–è€…æç¤ºç”¨æˆ·ç¡®è®¤æ¨¡å‹ç±»å‹ã€‚
                # ç›®å‰ llama-cpp-python å¯¹ Qwen2-VL çš„æ”¯æŒè¿˜åœ¨å®éªŒé˜¶æ®µã€‚
                # å¦‚æœæ˜¯ ggufï¼Œæœ‰äº›æ¨¡å‹å·²ç»å†…ç½®äº† projectorï¼Œä¸éœ€è¦é¢å¤–çš„ clipã€‚
                
                # å°è¯•åŠ è½½ Handlerï¼Œå¹¶æ•è·é”™è¯¯è€Œä¸å´©æºƒ
                try:
                    if Llava15ChatHandler:
                        chat_handler = Llava15ChatHandler(clip_model_path=clip_path)
                        print(f"\033[32m[UniversalGGUFLoader] Vision Adapter Loaded: {clip_model}\033[0m")
                    else:
                        print("\033[33m[UniversalGGUFLoader] Warning: Llava15ChatHandler missing.\033[0m")
                except Exception as e:
                    print(f"\033[31m[UniversalGGUFLoader] Failed to load CLIP handler (Llava15): {str(e)}\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Attempting to continue without CLIP handler (for models with built-in vision support or incompatible mmproj)...\033[0m")
                    chat_handler = None
            else:
                print(f"\033[33m[UniversalGGUFLoader] CLIP model not found: {clip_model}\033[0m")

        # [Auto-Detect Chat Format]
        # é’ˆå¯¹ Qwen ç­‰æ¨¡å‹ï¼Œè‡ªåŠ¨åº”ç”¨ chatml æ ¼å¼ï¼Œé¿å… llama-cpp-python çŒœé”™ã€‚
        # è¿™é‡Œè¿›è¡Œç®€å•çš„æ–‡ä»¶åå¯å‘å¼æ£€æµ‹ã€‚
        chat_format = None
        model_name = os.path.basename(model_path).lower()
        
        if "qwen" in model_name:
            chat_format = "chatml"
            print(f"\033[36m[UniversalGGUFLoader] Auto-detected Qwen model. Enforcing chat_format='chatml'.\033[0m")
        elif "llama-3" in model_name or "llama3" in model_name:
             chat_format = "llama-3"
        elif "vicuna" in model_name:
             chat_format = "vicuna"
        
        # å®ä¾‹åŒ–æ¨¡å‹
        model = Llama(
            model_path=model_path, 
            chat_handler=chat_handler,
            n_gpu_layers=n_gpu_layers, 
            n_ctx=n_ctx, 
            n_batch=512,
            chat_format=chat_format # æ³¨å…¥è‡ªåŠ¨è¯†åˆ«çš„æ ¼å¼
        )
        # æ ‡è®°æ˜¯å¦åŠ è½½äº† CLIPï¼Œä¾› Chat èŠ‚ç‚¹å‚è€ƒ
        model._loaded_clip_path = folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None
        # [Smart Vision Check] æ ‡è®°æ¨¡å‹æ˜¯å¦æ‹¥æœ‰æœ‰æ•ˆçš„ Vision Handler
        # è¿™å…è®¸ Chat èŠ‚ç‚¹åœ¨ç”¨æˆ·è¯¯è¿å›¾ç‰‡ä½†ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ï¼Œé¿å…æŠ¥é”™ã€‚
        model._has_vision_handler = chat_handler is not None
        # [Model Name] è®°å½•æ¨¡å‹æ–‡ä»¶åï¼Œç”¨äºåç»­çš„æ™ºèƒ½åˆ¤æ–­
        model._model_filename = os.path.basename(model_path)
        # [Smart Detection] Check if model is Qwen-based (for special prompt handling)
        model._is_qwen = "qwen" in os.path.basename(model_path).lower()
        
        return (model,)

# 3. æ ¸å¿ƒå¯¹è¯èŠ‚ç‚¹
# ==========================================================
# PROJECT: LoraHelper_Chat (DeepBlue Architecture)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. model (Loader) -> 2. image (Optional)
#   3. user_material (Material) -> 4. instruction (Command)
#   5. chat_mode (Logic Switch) -> 6. max_tokens -> 7. temperature
#   8. repetition_penalty -> 9. seed -> 10. release_vram
#
# LOGIC DEFINITION:
#   - user_material = Input Material
#   - instruction = Executive Instructions
#   - chat_mode = [Enhance_Prompt, Debug_Chat]
# ==========================================================
class UniversalAIChat:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("LLM_MODEL",), 
                "user_material": ("STRING", {"multiline": True, "default": DEFAULT_USER_MATERIAL}), 
                "instruction": ("STRING", {"multiline": True, "default": DEFAULT_INSTRUCTION}),
                "chat_mode": (["Enhance_Prompt", "Debug_Chat"],),
                "enable_tag": ("BOOLEAN", {"default": False, "label_on": "Enable Tags", "label_off": "Disable Tags"}),
                "enable_filename": ("BOOLEAN", {"default": False, "label_on": "Enable Filename", "label_off": "Disable Filename"}),
                "enable_cot": ("BOOLEAN", {"default": False, "label_on": "Enable Thinking (CoT)", "label_off": "Disable Thinking"}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.01}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.01}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "release_vram": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "image": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("prompt", "tags", "filename", "raw_output")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes"

    # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ (Force Execution)
    # é˜²æ­¢ ComfyUI å› ä¸ºè¾“å…¥æœªå˜ï¼ˆå¦‚å›ºå®š Seedï¼‰è€Œè·³è¿‡æ‰§è¡Œï¼Œå¯¼è‡´ç”¨æˆ·ä»¥ä¸ºâ€œæ²¡ååº”â€
    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def chat(self, model, user_material, instruction, chat_mode, enable_tag, enable_filename, enable_cot, max_tokens, temperature, repetition_penalty, seed, release_vram, image=None):
        # 0. åŸºç¡€é˜²å¾¡æ€§å¤„ç† (Defensive Check)
        if user_material is None: user_material = ""
        if instruction is None: instruction = ""
        
        # ==========================================================
        # 1. æ¨¡å¼åˆ¤å®šä¸é»˜è®¤æŒ‡ä»¤å®šä¹‰ (Mode Determination & Defaults)
        # ==========================================================
        
        # Widget Default Value (è§†ä¸ºâ€œç©ºâ€)
        WIDGET_DEFAULT_SC = ""

        # [Config] Constants moved to Global Scope (Top of file) for easy access.

        # Mode Logic
        # Priority: Image > Enhance > Debug
        is_vision_task = image is not None
        current_mode = "VISION" if is_vision_task else chat_mode # "Enhance_Prompt" or "Debug_Chat"
        
        # Check SC status
        sc_stripped = instruction.strip()
        is_sc_empty = (not sc_stripped) or (sc_stripped == WIDGET_DEFAULT_SC.strip())
        
        # Prepare Variables
        final_system_command = instruction
        final_user_content = "" # For text part
        apply_template = False
        
        # Mode Specific Logic
        if is_vision_task:
            # å¼ºåˆ¶ç»™ä¸€ä¸ªç®€çŸ­çš„ System è§’è‰²ï¼Œæœ‰æ—¶èƒ½æ¿€æ´» Qwen çš„å›å¤é€»è¾‘
            #messages.insert(0, {"role": "system", "content": "You are a helpful assistant that describes images in detail."})
            # [Vision Mode Guard]
            if not getattr(model, '_has_vision_handler', False):
                 raise ValueError("Vision Task detected (Image Input), but the loaded model does not have a Vision Handler (CLIP/MMProj). Please load a CLIP model in the Loader node.")

            # [Mode 1: Vision / Reverse Engineering]
            # Ignore UP (User Prompt is ignored as per request)
            # But we need an INSTRUCTION for the image.
            
            # Handle SC (System Command acts as the Instruction)
            if is_sc_empty:
                # No SC provided -> Use Fallback Instruction
                instruction_content = FALLBACK_VISION
                # Also set system command to this fallback for consistency? 
                # Or keep system command empty?
                # Usually system prompt defines "Who you are", User prompt defines "What to do".
                # For simplicity and effectiveness, we put the instruction in USER prompt.
                final_system_command = "" # Disable System Message for Vision
            else:
                # User provided SC -> Use it as the Instruction
                instruction_content = instruction
                # [FIX] Avoid duplication and System Role confusion in Vision Mode!
                # For Llama.cpp Vision, it's safest to use a SINGLE User Message containing [Image, Text].
                # We disable the System Message entirely for Vision tasks to prevent "0-token output" or handler errors.
                final_system_command = "" 
            
            # Set the content that goes into User Message
            final_user_content = instruction_content
            
            # Enable Template
            apply_template = True
            
        elif current_mode == "Enhance_Prompt":
            # [Mode 2: Prompt Enhance]
            # Use UP, wrapped with label
            final_user_content = f"{LABEL_USER_INPUT}\n{user_material}"
            
            # Handle SC
            if is_sc_empty:
                final_system_command = FALLBACK_ENHANCE
            else:
                final_system_command = instruction
                
            # Enable Template
            apply_template = True
            
        elif current_mode == "Debug_Chat":
            # [Mode 3: Debug]
            # Use UP directly (User should provide Context in UP if needed)
            final_user_content = user_material

            # Handle SC
            if is_sc_empty:
                final_system_command = FALLBACK_DEBUG
            else:
                final_system_command = instruction
            
            # Force Disable Switches
            enable_tag = False
            enable_filename = False
            enable_cot = True # Debug mode defaults to allowing thinking
            apply_template = False
            
        # ==========================================================
        # 2. æ¨¡æ¿æ„å»º (Template Construction)
        # ==========================================================
        template_instructions = ""
        
        # [Smart Template Logic]
        # Only apply rigid template if we actually need to extract specific parts (Tag/Filename).
        # If both are disabled, we should allow the model to flow naturally.
        needs_structure = enable_tag or enable_filename
        
        # We ignore 'apply_template' flag for content decision, only use it as a gate for modes that support it.
        # But effectively, if needs_structure is False, we append NOTHING.
        
        if apply_template:
            # [Strict Instruction Injection]
            # ç”¨æˆ·è¦æ±‚ï¼šæ— è®ºæ˜¯ç”¨æˆ·æŒ‡ä»¤è¿˜æ˜¯é»˜è®¤æŒ‡ä»¤ï¼Œéƒ½è¦åŠ ä¸Šâ€œä»…è¾“å‡ºæœ€ç»ˆæè¿°â€ã€â€œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹â€ã€â€œä¸è¦ç”Ÿæˆæ— æ•ˆæ–‡å­—â€ã€‚
            # è¿™å¿…é¡»ä½œä¸ºç³»ç»Ÿçº§çš„å¼ºåˆ¶çº¦æŸï¼Œè¿½åŠ åœ¨ System Command æˆ– User Prompt çš„æœ«å°¾ã€‚
            
            # [CoT Switch Logic]
            # If enable_cot is True, we SKIP the "No Thinking" constraint.
            # If enable_cot is False (default), we ENFORCE it.
            
            # [Smart Constraint] Dynamically append specific format instructions as Rules
            rules = []
            rules.extend(CONSTRAINT_NO_REPEAT)

            if not enable_cot:
                rules.extend(CONSTRAINT_NO_COT)
            else:
                rules.extend(CONSTRAINT_ALLOW_COT)

            if enable_tag:
                rules.append(PROMPT_TAGS)
            
            if enable_filename:
                rules.append(PROMPT_FILENAME)

            strict_constraints = CONSTRAINT_HEADER
            for i, rule in enumerate(rules, 1):
                strict_constraints += f"{i}. {rule}\n"
            
            # [Smart Constraint] Dynamically append output trigger based on switches
            output_order = [TRIGGER_ORDER_DESC]
            if enable_tag:
                output_order.append(TRIGGER_ORDER_TAGS)
            if enable_filename:
                output_order.append(TRIGGER_ORDER_FILENAME)
            
            start_sequence = f"\nä¸‹é¢å¼€å§‹è¾“å‡ºä½ çš„æœ€ç»ˆå†…å®¹ï¼Œå†…å®¹åŒ…å«ä»¥ä¸‹{len(output_order)}ä¸ªéƒ¨åˆ†ï¼Œè¯·æŒ‰é¡ºåºè¾“å‡ºä¸”ä»…è¾“å‡ºä¸‹åˆ—å†…å®¹ï¼š\n{chr(10).join(output_order)}{TRIGGER_SUFFIX}"
            strict_constraints += start_sequence

            # Append constraints to template_instructions (which is appended to User Message)
            # This ensures it's the LAST thing the model sees.
            template_instructions += strict_constraints
            
            # If not needs_structure, template_instructions only contains strict_constraints (if apply_template is True).
            
        # ==========================================================
        # 3. æ¶ˆæ¯ç»„è£… (Message Assembly)
        # ==========================================================
        is_qwen_model = getattr(model, '_is_qwen', False)
        
        messages = []
        # 3.1 System Message
        # [Fix] Some models require a System Message to initialize the context correctly, even if empty.
        # Especially Qwen-VL or Llama-3-Vision might expect the chat template to start with System.
        # If final_system_command is empty (Vision Mode), we skip adding it to avoid confusing the Handler?
        # User feedback suggests MISSING System message might be the cause of 0-token output.
        # Let's try adding a generic System Message if it's empty but we are in Vision Mode?
        # OR: Restore the generic system persona for Vision Mode, but keep it very simple.
        
        if final_system_command:
            messages.append({"role": "system", "content": final_system_command})
        # elif is_vision_task:
             # [Vision Fix] Qwen/Llama Vision often fail if System message is present
             # We strictly omit System message for Vision tasks to prevent 0-token output.
             # messages.append({"role": "system", "content": PROMPT_SYSTEM_DEFAULT})
    
        # 3.2 User Message
        if is_vision_task:
            # [Vision Mode]
            # Image Processing
            i = 255. * image[0].cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
            
            # [Format Handling]
            # Ensure RGB. Handle RGBA by pasting on white background (better for vision models than black default)
            if img.mode == "RGBA":
                background = Image.new("RGB", img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[3]) # 3 is the alpha channel
                img = background
            elif img.mode != "RGB":
                img = img.convert("RGB")
                
            buffered = BytesIO()
            # [Optimization] Use JPEG for better compatibility and smaller size
            # PNG can sometimes cause issues with certain VLM tokenizers or just be too large.
            # JPEG quality 95 is virtually lossless for vision tasks.
            # [Resize Logic]
            # If the image is too large, we should resize it to avoid OOM or excessive token usage.
            # Standard VLM limit is often around 1024x1024 or 2048x2048 (depending on model).
            # Qwen-VL handles high res well, but >2048 is usually diminishing returns for simple captioning.
            # Let's cap at 1536px on the long edge to be safe and fast.
            max_dimension = 1536
            if max(img.size) > max_dimension:
                scale_factor = max_dimension / max(img.size)
                new_size = (int(img.size[0] * scale_factor), int(img.size[1] * scale_factor))
                img = img.resize(new_size, Image.Resampling.LANCZOS)
                print(f"\033[36m[UniversalAIChat] Image Resized to {img.size}\033[0m")

            img.save(buffered, format="JPEG", quality=95) 
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            print(f"\033[36m[UniversalAIChat] Image Processed. Size: {img.size}, Mode: {img.mode} -> RGB/JPEG\033[0m")
            
            # User Content Construction
            # [Simplicity First]
            
            if is_sc_empty:
                prefix = "Instructions for the image above:\n"
            else:
                prefix = "\n" 
            
            user_text_content = f"{prefix}{final_user_content}\n{template_instructions}"
            
            # === DEBUG PROMPT DIFFERENCE ===
            print(f"\n\033[33m[UniversalAIChat] === PROMPT CONTENT DEBUG ===\033[0m")
            print(f"\033[33m[UniversalAIChat] SWITCHES: Tag={enable_tag}, Filename={enable_filename}\033[0m")
            print(f"\033[33m[UniversalAIChat] FINAL PROMPT SENT TO MODEL:\n----------------------------------------\n{user_text_content}\n----------------------------------------\033[0m\n")
            
            # Standard Multimodal Message Structure
            # Works for Llama-3-Vision, Qwen-VL, MiniCPM-V via llama-cpp-python
            user_content_list = [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}},
                {"type": "text", "text": user_text_content}
            ]
            
            messages.append({"role": "user", "content": user_content_list})
            display_up = f"[IMAGE]\n{user_text_content}"
            
            print(f"\033[36m[UniversalAIChat] Vision Prompt Constructed. Messages: {len(messages)}\033[0m")
            
        else:
            # [Text Mode (Enhance / Debug)]
            final_text_content = f"{final_user_content}{template_instructions}"
            messages.append({"role": "user", "content": final_text_content})
            
            display_up = f"ğŸ›¡ï¸ [System Instruction]:\n{final_system_command}\n\n{final_text_content}"

        # ==========================================================
        # 4. æ¨ç†æ‰§è¡Œ (Inference Execution)
        # ==========================================================
        
        # [State Management]
        # Vision models with adapters are sensitive to KV cache state.
        # We MUST reset the model state before each generation to prevent:
        # 1. "Turn off switch but still broken" (Cache corruption)
        # 2. Interference from previous turns
        # if hasattr(model, 'reset'):
        #     model.reset()
        
        # [Check for Released Model]
        if getattr(model, '_is_closed', False):
             print(f"\033[31m[UniversalAIChat] ğŸ”´ æ¨¡å‹å·²é‡Šæ”¾ (Model Released)\033[0m")
             print(f"\033[33m[UniversalAIChat] ğŸ’¡ æ‚¨ä¸Šæ¬¡è¿è¡Œå¼€å¯äº† 'release_vram'ï¼Œå¯¼è‡´æ¨¡å‹ä»æ˜¾å­˜ä¸­å¸è½½ã€‚\033[0m")
             print(f"\033[33m[UniversalAIChat] ğŸ’¡ è¯·ä¿®æ”¹ [LH_GGUFLoader] èŠ‚ç‚¹çš„ä»»æ„å‚æ•°ï¼ˆä¾‹å¦‚æ”¹å˜ n_ctx æˆ– n_gpu_layersï¼‰ï¼Œä»¥è§¦å‘æ¨¡å‹é‡æ–°åŠ è½½ã€‚\033[0m")
             raise ValueError("Model is closed (release_vram was active). Please reload the model by changing Loader parameters.")

        # Print Debug Info
        # print(f"\033[36m[UniversalAIChat] Mode: {current_mode}\033[0m")
        # print(f"\033[36m[UniversalAIChat] System Command: {final_system_command[:50]}...\033[0m")
        
        try:
            # [Optimization] Standard Stop Tokens
            # We strictly stick to standard EOS tokens to avoid "0-token output" caused by false positives.
            # Removing custom tokens like "[PART 1: Description]" because they might be the START of the generation!
            
            # [CRITICAL FIX] Stop Token Strategy for Vision
            # User diagnosis: "Stop Token hitting start" or "0 output".
            # For Vision tasks, especially with Qwen2-VL or Llama-3-Vision via GGUF,
            # explicit stop tokens might be triggering false positives if the chat template is slightly mismatched.
            # We will disable explicit stop tokens for Vision tasks and rely on the model's EOS.
            # [Unified Inference Setup]
            if is_vision_task:
                # Vision Mode
                # 1. Stop Tokens: Disable explicit stop tokens to prevent false positives (0-token output).
                #    Vision models (like Qwen-VL) often trigger stop tokens prematurely if we force them.
                stop_tokens = None
                
                # 2. Repetition Penalty:
                # - If structure is needed (Tags/Filename), use mild penalty (1.05) to prevent infinite tag loops.
                # - If no structure, use 1.0 (no penalty) to allow natural captioning flow.
                repetition_penalty = 1.05 if (apply_template and needs_structure) else 1.0
                
                # Debug
                if apply_template and needs_structure:
                     print(f"\033[36m[UniversalAIChat] Vision + Structure: Penalty={repetition_penalty}, StopTokens=None\033[0m")

            else:
                # Text Mode (Enhance / Chat)
                # 1. Stop Tokens: Use standard ChatML/Llama stop tokens. Text models rely on these to stop.
                #    Without this, complex instructions (like "5 sections") cause the model to loop or hallucinate.
                stop_tokens = ["<|im_end|>", "<|endoftext|>"]
                
                # 2. Repetition Penalty:
                # - Always use mild penalty (1.1) for text enhancement to prevent loops.
                repetition_penalty = 1.1
                
                print(f"\033[36m[UniversalAIChat] Text Mode: Penalty={repetition_penalty}, StopTokens={stop_tokens}\033[0m")

            safe_temperature = min(max(temperature, 0.0), 2.0)
            
            # Vision Task uses create_chat_completion (mandatory for image handler)
            # Text Task also uses it now for consistency, unless specific Qwen issues arise.
            # (Previously we switched to manual ChatML for Text to avoid Llama template errors, but Qwen handles standard messages well if chat_format is set)
            
            # [Unified Inference]
            # Use create_chat_completion for both Text and Vision tasks.
            # This ensures compatibility with whatever chat_format is detected (ChatML, Llama-3, Vicuna, etc.)
            
            output = model.create_chat_completion(
                messages=messages, 
                max_tokens=max_tokens, 
                temperature=safe_temperature, 
                repeat_penalty=repetition_penalty, 
                seed=seed,
                stop=stop_tokens
            )
            if not output or 'choices' not in output or not output['choices']:
                 raise ValueError("Empty response from model.")
            full_res = output['choices'][0]['message']['content']
            finish_reason = output['choices'][0].get('finish_reason', 'unknown')
            usage = output.get('usage', {})



            
            print(f"\033[36m[UniversalAIChat] Usage: {usage}, Finish Reason: {finish_reason}\033[0m")
            
            if finish_reason == 'length':
                print(f"\033[31m[UniversalAIChat] WARNING: Output Truncated! Max Tokens or Context Limit Reached.\033[0m")
                print(f"\033[33m[UniversalAIChat] Solution 1: Increase 'max_tokens' in THIS node (Chat) - likely the cause.\033[0m")
                print(f"\033[33m[UniversalAIChat] Solution 2: Increase 'n_ctx' in Loader node (if input is very long).\033[0m")
                full_res += "\n\n[SYSTEM: Output Truncated. Please increase 'max_tokens' (Chat Node) or 'n_ctx' (Loader Node).]"
            
            # [Post-Processing] æ¸…ç†å¯èƒ½æ®‹ç•™çš„ Token
            if full_res:
                 for token in ["[/INST]", "[INST]", "<|im_end|>", "<|endoftext|>", "<|im_start|>"]:
                     full_res = full_res.replace(token, "")
            
            # [Anti-Repetition Guard]
            # æ£€æµ‹å¹¶ç§»é™¤ System Command å¤è¯»
            # å¦‚æœ full_res ä»¥ system_command å¼€å¤´ï¼ˆå…è®¸å°‘é‡å·®å¼‚ï¼‰ï¼Œåˆ™ç§»é™¤
            if final_system_command and len(final_system_command) > 10:
                # ç®€å•çš„å‰ç¼€æ£€æŸ¥
                if full_res.strip().startswith(final_system_command.strip()[:20]):
                    print(f"\033[33m[UniversalAIChat] Warning: System Command repetition detected at start. Attempting to clean...\033[0m")
                    # å°è¯•æ‰¾åˆ° System Command çš„ç»“æŸä½ç½®
                    # è¿™é‡Œå‡è®¾ System Command æ˜¯å®Œæ•´çš„
                    if final_system_command.strip() in full_res:
                        temp_res = full_res.replace(final_system_command.strip(), "", 1).strip()
                        if temp_res:
                            full_res = temp_res
                        else:
                            # å¦‚æœç§»é™¤åä¸ºç©ºï¼Œè¯´æ˜æ¨¡å‹åªæ˜¯å¤è¯»äº†æŒ‡ä»¤
                            # è¿™ç§æƒ…å†µä¸‹ï¼Œä¿ç•™åŸå†…å®¹å¯èƒ½æ›´å¥½ï¼Œè®©ç”¨æˆ·çœ‹åˆ°â€œå®ƒå¤è¯»äº†â€ï¼Œè€Œä¸æ˜¯â€œå®ƒæ²¡è¯´è¯â€
                            print(f"\033[31m[UniversalAIChat] Warning: Model only repeated the instruction!\033[0m")
                            # full_res = "[Error: Model only repeated the instruction]" # Optional
                            pass 
                    else:
                        # å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…ï¼Œå¯èƒ½æ˜¯å› ä¸º Tokenization å¯¼è‡´çš„å¾®å°å·®å¼‚
                        pass
            
            if not full_res:
                 # å°è¯•è·å– finish_reasonï¼Œçœ‹æ˜¯å¦æ˜¯å› ä¸º token é™åˆ¶æˆ–å…¶ä»–åŸå› æˆªæ–­
                 print(f"\033[33m[UniversalAIChat] Empty Content. Finish Reason: {finish_reason}\033[0m")
                 
        except Exception as e:
            error_msg = str(e)
            full_res = f"Error: {error_msg}"
            print(f"\033[31m[UniversalAIChat] Generation Error: {error_msg}\033[0m")
            
            # [Friendly Error Handler]
            # é’ˆå¯¹å¸¸è§çš„ "No KV slot available" é”™è¯¯ç»™å‡ºä¸­æ–‡å»ºè®®
            if "No KV slot available" in error_msg:
                 print(f"\033[31m[UniversalAIChat] ğŸ”´ é”™è¯¯è¯Šæ–­: ä¸Šä¸‹æ–‡é•¿åº¦ (n_ctx) ä¸è¶³ï¼\033[0m")
                 print(f"\033[33m[UniversalAIChat] ğŸ’¡ è§£å†³æ–¹æ¡ˆ: è¯·åœ¨ [Qwen3_GGUF_loader] èŠ‚ç‚¹ä¸­ï¼Œå°† 'n_ctx' çš„å€¼è°ƒå¤§ã€‚\033[0m")
                 print(f"\033[33m[UniversalAIChat]    - å½“å‰å¯èƒ½è®¾ç½®è¿‡å°ï¼Œå»ºè®®å°è¯• 8192, 16384 æˆ– 32768ã€‚\033[0m")
                 print(f"\033[33m[UniversalAIChat]    - è§†è§‰ä»»åŠ¡(Vision)é€šå¸¸éœ€è¦æ›´å¤§çš„ä¸Šä¸‹æ–‡ç©ºé—´ã€‚\033[0m")
                 
                 full_res += "\n\n[SYSTEM ERROR]: Context Window Full (n_ctx too small). Please increase 'n_ctx' in the Loader node."

        # 4. è¾“å‡ºè§£æ (Output Parsing)
        # ==========================================================
        
        # [Critical Correction] Monitor æ•°æ®æµ
        # Chat åº”è¯¥æŠŠâ€œæ–‡æœ¬åŸæ ·ä¸åŠ¨â€ç»™ Monitorï¼Œè¿ think è¿‡ç¨‹éƒ½è¦ä¿ç•™ã€‚
        raw_output = f"User: {display_up}\nAI: {full_res}"

        if release_vram:
            # [Fix] Explicitly close the llama.cpp model to release VRAM
            # Simply gc.collect() is NOT enough for C++ bound objects.
            try:
                if hasattr(model, 'close'):
                    model.close()
                    print("\033[36m[UniversalAIChat] ğŸ§¹ Model Closed (VRAM Released).\033[0m")
            except Exception as e:
                print(f"\033[33m[UniversalAIChat] Warning during model close: {e}\033[0m")
            
            # Mark as closed so we can warn user next time
            model._is_closed = True
            
            gc.collect()
            torch.cuda.empty_cache()

        # 5. ç®€å•åˆ†å‰²é€»è¾‘ (Simple Splitter)
        # æ—¢ç„¶ Splitter èŠ‚ç‚¹å·²åˆ é™¤ï¼Œè¿™é‡Œå¿…é¡»æ‰¿æ‹…èµ·åˆ†å‰²çš„ä»»åŠ¡ã€‚
        # é…åˆæ–°çš„ Trigger æ ¼å¼ï¼š**description**:, **tags**:, **filename**:
        
        # Step A: æ¸…ç† <think> æ ‡ç­¾ (ä»…é’ˆå¯¹ç»“æ„åŒ–è¾“å‡ºç«¯å£)
        clean_res = re.sub(r'<think>.*?</think>', '', full_res, flags=re.DOTALL).strip()
        
        # å¤„ç†æœªé—­åˆçš„ <think>
        if '<think>' in clean_res:
            clean_res = clean_res.split('<think>')[0].strip()
            
        # Step B: å®šä¹‰æ ‡è®° (Markers)
        # å¿…é¡»ä¸ Trigger å®šä¹‰ä¿æŒä¸€è‡´ (å¿½ç•¥å¤§å°å†™)
        marker_desc = "**description**:"
        marker_tags = "**tags**:"
        marker_filename = "**filename**:"
        
        # è¾…åŠ©å‡½æ•°ï¼šæŸ¥æ‰¾ä½ç½®
        def get_pos(marker, text):
            m = re.search(re.escape(marker), text, re.IGNORECASE)
            return m.start() if m else -1
            
        pos_desc = get_pos(marker_desc, clean_res)
        pos_tags = get_pos(marker_tags, clean_res)
        pos_filename = get_pos(marker_filename, clean_res)
        
        # Step C: æå– Description
        # é€»è¾‘ï¼š
        # 1. å¦‚æœæ‰¾åˆ° **description**:ï¼Œä»å®ƒåé¢å¼€å§‹ã€‚
        # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé»˜è®¤ä»å¤´å¼€å§‹ã€‚
        # 3. æˆªæ­¢åˆ° **tags**: æˆ– **filename**: (è°åœ¨å‰ç®—è°)ã€‚
        
        start_desc = 0
        if pos_desc != -1:
            start_desc = pos_desc + len(marker_desc)
            
        end_desc = len(clean_res)
        candidates = []
        if pos_tags != -1 and pos_tags > start_desc: candidates.append(pos_tags)
        if pos_filename != -1 and pos_filename > start_desc: candidates.append(pos_filename)
        
        if candidates:
            end_desc = min(candidates)
            
        out_desc = clean_res[start_desc:end_desc].strip()
        
        # Step D: æå– Tags
        out_tags = ""
        if enable_tag and pos_tags != -1:
            start_tags = pos_tags + len(marker_tags)
            end_tags = len(clean_res)
            # å¦‚æœ filename åœ¨ tags åé¢ï¼Œåˆ™æˆªæ­¢åˆ° filename
            if pos_filename != -1 and pos_filename > start_tags:
                end_tags = pos_filename
            
            raw_tags = clean_res[start_tags:end_tags].strip()
            # ç®€å•æ¸…ç†ï¼šæ¢è¡Œå˜é€—å·
            out_tags = raw_tags.replace("\n", ",")
            
        # Step E: æå– Filename
        out_filename = ""
        if enable_filename and pos_filename != -1:
             start_fn = pos_filename + len(marker_filename)
             raw_fn = clean_res[start_fn:].strip()
             # æå–ä¸­æ‹¬å·å†…å®¹
             m = re.search(r'\[(.*?)\]', raw_fn)
             if m:
                 out_filename = m.group(1)
             else:
                 out_filename = raw_fn.split('\n')[0] # æ²¡æ‹¬å·å°±å–ç¬¬ä¸€è¡Œ

        # ==========================================================
        # 6. è¾“å‡ºç»“æœ (Return)
        # ==========================================================
        return (out_desc, out_tags, out_filename, raw_output)

# 4. å†å²ç›‘æ§èŠ‚ç‚¹ (æµæ°´çº¿æ’åº)
# ==========================================================
# PROJECT: LoraHelper_Monitor (History Viewer)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. raw_output (Raw Text Input)
#
# LOGIC DEFINITION:
#   - Maintains a rolling buffer of last 5 chat interactions
#   - Output 1: context (Raw text for LLM)
#   - UI Display: Formatted cards (Old -> New)
# ==========================================================
class LH_History_Monitor:
    def __init__(self):
        self.history = []

    @classmethod
    def INPUT_TYPES(s):
        return { 
            "required": { 
                "raw_input": ("STRING", {"forceInput": True}),
                "clear_history": ("BOOLEAN", {"default": False, "label_on": "Clear History", "label_off": "Keep History"})
            } 
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    OUTPUT_NODE = True
    FUNCTION = "update"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def update(self, raw_input, clear_history):
        # 0. Clear History Check
        if clear_history:
            self.history = []
            # We still process the current input, but it will be the ONLY item in history.
            print("\033[36m[LH_History_Monitor] History Cleared by User.\033[0m")
        # 1. è§£æè¾“å…¥ (æ”¯æŒ JSON æˆ– çº¯æ–‡æœ¬)
        import json
        user_msg = ""
        ai_msg = ""
        
        # å°è¯•è§£æç‰¹å®šæ ¼å¼ "User: ... \nAI: ..."
        if isinstance(raw_input, str) and raw_input.startswith("User:"):
             # ä½¿ç”¨ split åˆ†å‰²ï¼Œæ³¨æ„åªåˆ†å‰²ç¬¬ä¸€ä¸ª "\nAI: "
             parts = raw_input.split("\nAI: ", 1)
             if len(parts) == 2:
                 user_msg = parts[0][5:].strip() # å»æ‰ "User: "
                 ai_msg = parts[1].strip()
             else:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        else:
            try:
                data = json.loads(raw_input)
                if isinstance(data, dict):
                    user_msg = data.get("user", "")
                    ai_msg = data.get("ai", "")
                else:
                    user_msg = "Raw Input"
                    ai_msg = str(raw_input)
            except:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        
        # 2. æ›´æ–°å†å² (å»é‡)
        # æ„é€ ä¸€ä¸ªç»“æ„åŒ–å¯¹è±¡å­˜å‚¨
        new_entry = {"user": user_msg, "ai": ai_msg}
        
        # ç®€å•å»é‡ï¼šæ£€æŸ¥ä¸Šä¸€æ¡æ˜¯å¦å®Œå…¨ä¸€è‡´
        if self.history:
            last = self.history[-1]
            if last["user"] == user_msg and last["ai"] == ai_msg:
                pass # é‡å¤ï¼Œå¿½ç•¥
            else:
                self.history.append(new_entry)
        else:
            self.history.append(new_entry)
            
        # ä¿æŒ 5 è½®
        if len(self.history) > 5:
            self.history.pop(0)

        # 3. æ„é€  Context (ç”¨äºå›ä¼ ç»™ Chat)
        # æ ¼å¼ï¼šRound X User: ... \n Round X AI: ...
        context_parts = []
        for i, h in enumerate(self.history):
            context_parts.append(f"Round {i+1} User: {h['user']}")
            context_parts.append(f"Round {i+1} AI: {h['ai']}")
        context = "\n\n".join(context_parts)

        # 4. æ„é€  UI æ˜¾ç¤º â€”â€” å…³é”®ä¿®æ”¹ï¼šæ‹†åˆ†æˆå¤šä¸ªçŸ­æ–‡æœ¬å—
        ui_text = []
        ui_text.append("â•â•â•â•â•â•â•â•â• ğŸ‘€ Visual History (Latest 5 Rounds) â•â•â•â•â•â•â•â•â•\n")
        
        for i, h in enumerate(reversed(self.history)): # æœ€æ–°è½®åœ¨ä¸Š
            idx = len(self.history) - i
            ui_text.append(f"ğŸ”» Round {idx} â€” User è¾“å…¥")
            ui_text.append(h["user"] or "(ç©º)")  # å•ç‹¬ä¸€å—ï¼Œç”¨æˆ·è¾“å…¥
            
            ui_text.append(f"ğŸ”¹ Round {idx} â€” AI è¾“å‡º")
            ui_text.append(h["ai"] or "(ç©º)")    # å•ç‹¬ä¸€å—ï¼ŒAIè¾“å‡º
            
            ui_text.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")  # åˆ†éš”çº¿

        # å¦‚æœå†å²ä¸ºç©º
        if len(ui_text) <= 1:
             ui_text.append("ï¼ˆæš‚æ— å¯¹è¯å†å²ï¼‰")
        
        return {"ui": {"text": ui_text}, "result": (context,)}