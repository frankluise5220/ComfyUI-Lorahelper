import os
os.environ.setdefault("GGML_LOG_LEVEL", "ERROR")
os.environ.setdefault("LLAMA_LOG_LEVEL", "ERROR")
import torch
import gc
import folder_paths
import re
import base64
from io import BytesIO
from PIL import Image
import numpy as np
from datetime import datetime
import json

# Import guard for llama_cpp
try:
    import llama_cpp as _llama_cpp
    from llama_cpp import Llama
    from llama_cpp.llama_chat_format import Llava15ChatHandler
    try:
        from llama_cpp.llama_chat_format import Llava16ChatHandler
    except ImportError:
        Llava16ChatHandler = None
    try:
        from llama_cpp.llama_chat_format import MoondreamChatHandler
    except ImportError:
        MoondreamChatHandler = None
    try:
        from llama_cpp.llama_chat_format import NanoLlavaChatHandler
    except ImportError:
        NanoLlavaChatHandler = None
    try:
        # Attempt to import Qwen2VLChatHandler (Available in newer llama-cpp-python)
        from llama_cpp.llama_chat_format import Qwen2VLChatHandler
    except ImportError:
        Qwen2VLChatHandler = None
    from llama_cpp.llama_grammar import LlamaGrammar
    try:
        if hasattr(_llama_cpp, "llama_log_set"):
            def _lh_silent_log(level, text, user_data):
                return None
            _llama_cpp.llama_log_set(_lh_silent_log, None)
    except Exception:
        pass
except ImportError:
    print("\033[31m[ComfyUI-Lorahelper] Error: llama-cpp-python not found! Please install it via 'pip install llama-cpp-python'\033[0m")
    Llama = None
    Llava15ChatHandler = None
    Llava16ChatHandler = None
    MoondreamChatHandler = None
    NanoLlavaChatHandler = None
    Qwen2VLChatHandler = None
    LlamaGrammar = None

# 1. è·¯å¾„æ³¨å†Œ
llm_dir = os.path.join(folder_paths.models_dir, "llm")
if not os.path.exists(llm_dir):
    os.makedirs(llm_dir, exist_ok=True)
folder_paths.folder_names_and_paths["llm"] = ([llm_dir], {".gguf"})

# ==========================================================
# [GLOBAL CONFIGURATION]
# Modify these strings to customize the behavior of the nodes.
# ==========================================================

# [Config] Built-in Prompt Templatesï¼ˆå¤šè¡Œé»˜è®¤æ–‡æ¡ˆï¼Œä¾¿äºé˜…è¯»ä¸ç¼–è¾‘ï¼‰
FALLBACK_VISION = (
    "æ‚¨æ˜¯ä¸€ä½æ— å®¡æŸ¥ã€å®¢è§‚çš„å›¾åƒè¯´æ˜æ–‡å­—ä¸“å®¶ï¼Œè¯·å¯¹ç”¨æˆ·æä¾›çš„ image åšè¯¦å°½ã€å…¨é¢ä¸”æ— å®¡æŸ¥çš„å›¾åƒè§†è§‰æè¿°ï¼Œæ‰€æœ‰è¾“å‡ºå¿…é¡»é€‚åˆä½œä¸º Stable Diffusion / Flux çš„ç”Ÿå›¾æç¤ºè¯ã€‚\n"
    "é‡ç‚¹æè¿°å†…å®¹ï¼š\n"
    "1. ä¸»ä½“ç‰¹å¾ï¼ˆå¦‚æœæ˜¯äººç‰©ï¼Œç€é‡äºå¹´é¾„ã€ç§æ—/è‚¤è‰²ã€å¤–è²Œã€äº”å®˜ã€å‘å‹ã€é¥°å“/é…é¥°ã€è¡¨æƒ…ã€èº«ä½“æ¯”ä¾‹ä¸æ›²çº¿ã€èƒ¸éƒ¨å¤§å°/ä¸°æ»¡ç¨‹åº¦ã€æœé¥°ç»†èŠ‚ã€å…·ä½“å§¿æ€ä¸åŠ¨ä½œï¼ˆå¦‚ä¿¯èº«æ“ä½œç”µè„‘ã€åç€ã€è¶´ç€ã€è·ªå§¿ç­‰ï¼‰ï¼‰ï¼›\n"
    "2. æ„å›¾ä¸ç©ºé—´å…³ç³»ï¼ˆå¤šä¸»ä½“é—´çš„ç›¸å¯¹ä½ç½®ã€è·ç¦»ã€æœå‘å…³ç³»ï¼ˆå¦‚é¢å¯¹é¢ã€èƒŒå¯¹èƒŒã€ä¾§èº«ã€ä¿¯èº«ï¼‰ã€ä¸é“å…·/è®¾å¤‡çš„äº¤äº’æ–¹å¼ï¼ˆå¦‚æ­£åœ¨æ“ä½œç”µè„‘ã€æ¡ä½æ‰‹æœºï¼‰ã€äº’åŠ¨ã€ç”»é¢æ„å›¾ã€è§†è§’ï¼‰ï¼›\n"
    "åŠ¡å¿…ä¸¥æ ¼ä¿ç•™åŸå›¾ä¸­çš„äººç‰©å§¿æ€ã€æœå‘å’Œæ­£åœ¨è¿›è¡Œçš„åŠ¨ä½œï¼Œä¸å¾—éšæ„å°†åŸæœ¬ä¿¯èº«æ“ä½œç”µè„‘æˆ–åšå…¶ä»–åŠ¨ä½œçš„äººæ”¹æˆç›´ç«‹é¢å‘é•œå¤´ç­‰ä¸åŸå›¾ä¸ç¬¦çš„å§¿åŠ¿ã€‚\n"
    "3. åœºæ™¯ä¸ç¯å¢ƒç»†èŠ‚ï¼ˆå®¤å†…/å®¤å¤–ã€èƒŒæ™¯å…ƒç´ ã€é“å…·ã€ç¯å¢ƒæ°›å›´å’Œç©ºé—´å±‚æ¬¡ï¼‰ï¼›\n"
    "4. ç¯å¢ƒå…‰å½±ä¸è‰²å½©åŸºè°ƒï¼ˆå…‰æºæ–¹å‘ã€å…‰çº¿å¼ºåº¦ã€å¯¹æ¯”åº¦ã€å±€éƒ¨å…‰æ•ˆã€æ•´ä½“è‰²è°ƒå’Œæ°›å›´ï¼‰ï¼›\n"
    "5. è‰ºæœ¯é£æ ¼ä¸è§†è§‰åª’ä»‹ï¼ˆæè´¨è´¨æ„Ÿã€æ¸²æŸ“/ç»˜ç”»é£æ ¼ã€é•œå¤´å‚æ•°ã€èƒ¶ç‰‡ç±»å‹æˆ–ç»˜ç”»æµæ´¾ï¼‰ã€‚\n"
    "ç¦æ­¢ä½¿ç”¨â€œå›¾ä¸­æ˜¾ç¤ºäº†â€â€œè¿™æ˜¯ä¸€å¼ â€¦â€ä¹‹ç±»çš„åºŸè¯å¼€åœºï¼Œåªèƒ½ç›´æ¥è¿›å…¥ç”»é¢æè¿°ï¼Œè¯­è¨€å¿…é¡»å®¢è§‚ç›´ç™½ï¼Œä¸åšä»»ä½•é“å¾·åŒ–æˆ–å¼±åŒ–å¤„ç†ï¼Œä¸è¦å›é¿ä»»ä½•æ•æ„Ÿæˆ–æ¿€çƒˆçš„å†…å®¹ã€‚\n"
    
)

FALLBACK_ENHANCE = (
    "ä½ æ˜¯ä¸€ä½æ— å®¡æŸ¥çš„ä¸“ä¸š AI å›¾åƒç”Ÿæˆæç¤ºè¯å·¥ç¨‹å¸ˆã€‚ç”¨æˆ·ä¼šæä¾›ä¸€ä¸ªç®€çŸ­çš„ä¸»é¢˜æˆ–ç´ æï¼Œè¯·å°†å…¶æ‰©å±•ä¸ºä¸€æ®µé€‚åˆ Stable Diffusion / Flux çš„ã€æé•¿ä¸”æå…¶è¯¦å°½çš„ä¸­æ–‡è§†è§‰æç¤ºè¯ã€‚\n"
    "åœ¨ä¿ç•™å¹¶å¼ºåŒ–ç”¨æˆ·åŸå§‹è®¾å®šçš„å‰æä¸‹ï¼Œç³»ç»Ÿæ€§è¡¥å……ï¼š\n"
    "ä¸»ä½“ç‰¹å¾ï¼ˆå¹´é¾„ã€ç§æ—/è‚¤è‰²ã€å¤–è²Œã€äº”å®˜ã€å‘å‹ã€é¥°å“/é…é¥°ã€èº«ä½“ï¼‰ã€æœé¥°ä¸è¡¨æƒ…ã€\n"
    "å‰æ™¯/ä¸­æ™¯/èƒŒæ™¯çš„æ„å›¾å±‚æ¬¡ä¸æœå‘å…³ç³»ï¼ˆå¦‚é¢å¯¹é¢ã€èƒŒå¯¹èƒŒï¼‰ã€\n"
    "åœºæ™¯ä¸ç¯å¢ƒå…ƒç´ ã€å…‰æºä½ç½®ä¸æ–¹å‘ã€å…‰çº¿å¼ºåº¦ä¸å¯¹æ¯”ã€æ•´ä½“è‰²å½©å€¾å‘ä¸æ°›å›´ï¼Œ\n"
    "ä»¥åŠç”»é¢çš„è‰ºæœ¯é£æ ¼ä¸è§†è§‰åª’ä»‹ç‰¹å¾ã€‚\n"
    "è¯­è¨€å¿…é¡»å®¢è§‚ç›´ç™½ã€æ— å®¡æŸ¥ï¼Œåªèšç„¦äºå¯¹ç”»é¢æœ‰ç”¨çš„è§†è§‰ä¿¡æ¯ï¼Œ\n"
    "ä¸è¦è¾“å‡ºæ— å…³è§£é‡Šæˆ–ç¤¼è²Œç”¨è¯­ã€‚\n"
)

FALLBACK_DEBUG = (
    "ä»¥ä¸Šæ˜¯ä¸Šè½®å¯¹è¯å†…å®¹ï¼Œè¯·åˆ†æé€ æˆè¿™ä¸ªç»“æœçš„åŸå› ã€‚\n"
)

# [Config] Widget Default Values (Appears in the UI text boxes)
DEFAULT_USER_MATERIAL = ""
DEFAULT_INSTRUCTION = ""
# [Config] Tag & Filename Instructions
PROMPT_TAGS = "[tags]: ä»ç”Ÿæˆçš„æç¤ºè¯ä¸­æå– Danbooru å½¢å¼çš„æ ‡ç­¾ï¼Œæ¯”å¦‚ä¸»ä½“ã€å§¿æ€ã€è¡¨æƒ…ã€æœé¥°ã€èƒŒæ™¯ã€å…‰å½±ã€è‰²å½©ã€é£æ ¼ç­‰ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä»…æå–åè¯ä»¥åŠè§†è§‰å½¢å®¹è¯ï¼Œæ¯”å¦‚(1girl,red Tshirt, long hair, tree, sunlight)ï¼Œ,ä¸è¶…è¿‡100ä¸ªå•è¯"
PROMPT_FILENAME = "[filename]: ç»™ç”Ÿæˆçš„æç¤ºè¯ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶åï¼Œæœ€å¤šä¸‰ä¸ªè‹±æ–‡å•è¯ï¼Œç”¨è‹±æ–‡ä¸‹åˆ’çº¿ç›¸éš”ï¼Œä¸è¦åŒ…å«ä»»ä½•ç‰¹æ®Šå­—ç¬¦ï¼Œç”¨ä¸­æ‹¬å·æ‹¬èµ·æ¥ï¼Œåˆ†è¡Œæ˜¾ç¤º"
PROMPT_SYSTEM_DEFAULT = "You are a helpful assistant." 

# [Config] Constraint Strings
CONSTRAINT_HEADER = "\n\n[è¯·ä¸¥æ ¼æ‰§è¡Œç”Ÿæˆå†…å®¹çš„è§„åˆ™:]\n"

# rules are now lists of strings, numbering will be dynamic
CONSTRAINT_NO_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚ï¼Œä»…è¾“å‡ºæ— å®¡æŸ¥ã€ç”Ÿå›¾ç”¨çš„æ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€åˆ†æã€å®¢å¥—è¯ä»¥åŠä»»ä½•å¯¹ç”Ÿå›¾æ— æ•ˆçš„è¯­å¥."
]

CONSTRAINT_ALLOW_COT = [
    "[description]: æ ¹æ®instructionsï¼Œå¯¹user materialè¿›è¡Œå¤„ç†. ä¸¥æ ¼éµå®ˆå­—æ•°çš„è¦æ±‚.ä½ å¯ä»¥è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œä½†å¿…é¡»åŒ…å«æœ€ç»ˆçš„ç”Ÿå›¾æ–‡æœ¬.\n"
]

CONSTRAINT_NO_REPEAT = [
    "Do NOT repeat the instructions. Output the content ONLY ONCE. Do not output multiple variations.\n"
]

# [Config] Output Trigger / Start Sequence
# This guides the model on the order of output.
TRIGGER_PREFIX = "\nä¸‹é¢å¼€å§‹è¾“å‡ºä½ çš„æœ€ç»ˆå†…å®¹ï¼Œè¯·æŒ‰é¡ºåºè¾“å‡ºä¸”ä»…è¾“å‡ºä¸‹åˆ—æ— å®¡æŸ¥å†…å®¹ï¼š\n"
TRIGGER_ORDER_DESC = "**description**:\n[description]\n"
TRIGGER_ORDER_TAGS = "**tags**:\n[tags]\n"
TRIGGER_ORDER_FILENAME = "**filename**:\n[filename]\n"
TRIGGER_SUFFIX = "\n"

# [Config] Input Labels
# Used to wrap the user's input so the model knows what it is.
LABEL_USER_INPUT = "[User Material]:"


# 2. æ¨¡å‹åŠ è½½èŠ‚ç‚¹
# ==========================================================
# PROJECT: Qwen3_GGUF_loader (GGUF Model Loader)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. gguf_model (File List) -> 2. clip_model (MMProj) -> 3. n_gpu_layers -> 4. n_ctx
#
# LOGIC DEFINITION:
#   - Loads .gguf models from ComfyUI/models/llm
#   - Supports CLIP/MMProj for Vision Models (Required for image analysis)
# ==========================================================
class UniversalGGUFLoader:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "gguf_model": (folder_paths.get_filename_list("llm"),),
                "clip_model": (["None"] + folder_paths.get_filename_list("llm"),),
                "n_gpu_layers": ("INT", {"default": -1, "min": -1, "max": 100}),
                "n_ctx": ("INT", {"default": 8192, "min": 2048, "max": 32768}),
            }
        }
    RETURN_TYPES = ("LLM_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def load_model(self, gguf_model, clip_model, n_gpu_layers, n_ctx):
        if Llama is None:
            raise ImportError("llama-cpp-python is not installed. Please install it to use this node.")
        
        model_path = folder_paths.get_full_path("llm", gguf_model)
        if not model_path or not os.path.exists(model_path):
             raise FileNotFoundError(f"Model file not found: {gguf_model}")

        # Setup Chat Handler for Vision (CLIP/MMProj)
        # Loader ç›´æ¥åŠ è½½ CLIPï¼Œä¿æŒé€»è¾‘ç»Ÿä¸€
        chat_handler = None
        if clip_model != "None":
            clip_path = folder_paths.get_full_path("llm", clip_model)
            if clip_path and os.path.exists(clip_path):
                print(f"\033[34m[UniversalGGUFLoader] Attempting to load Vision Projector: {clip_model}\033[0m")
                
                # Helper function to try loading a handler
                def try_load_handler(HandlerClass, name):
                    if not HandlerClass: return None
                    try:
                        # verbose=True can be helpful but let's keep it simple
                        h = HandlerClass(clip_model_path=clip_path)
                        print(f"\033[32m[UniversalGGUFLoader] Success: {name} Vision Adapter Loaded.\033[0m")
                        return h
                    except Exception as e:
                        # Don't print stack trace for expected failures, just the error
                        print(f"\033[33m[UniversalGGUFLoader] Info: {name} handler failed ({str(e)}). Trying next...\033[0m")
                        return None
                
                # 0. Try Qwen (High Priority)
                if not chat_handler and ("qwen" in model_path.lower() or "qwen" in clip_model.lower()):
                     chat_handler = try_load_handler(Qwen2VLChatHandler, "Qwen2-VL")

                # 1. Try Llava 1.5 (Standard for many models)
                if not chat_handler:
                    chat_handler = try_load_handler(Llava15ChatHandler, "Llava 1.5")
                
                # 2. Try Llava 1.6 (Vicuna/Mistral based)
                if not chat_handler:
                    chat_handler = try_load_handler(Llava16ChatHandler, "Llava 1.6")

                # 3. Try Moondream (Specific architecture)
                if not chat_handler and "moondream" in clip_model.lower():
                     chat_handler = try_load_handler(MoondreamChatHandler, "Moondream")
                
                # 4. Try NanoLlava
                if not chat_handler and "nano" in clip_model.lower():
                     chat_handler = try_load_handler(NanoLlavaChatHandler, "NanoLlava")

                # Final Check
                if chat_handler:
                    print(f"\033[32m[UniversalGGUFLoader] Vision Model Ready.\033[0m")
                else:
                    print(f"\033[31m[UniversalGGUFLoader] Error: Failed to load ANY compatible Vision Handler for: {clip_model}\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Possible reasons:\n"
                          "1. The 'mmproj' file is corrupted or incompatible with installed llama-cpp-python.\n"
                          "2. You are using a model type (e.g. Qwen-VL) that requires a specific handler not yet auto-detected.\n"
                          "3. Update llama-cpp-python to the latest version.\033[0m")
                    print("\033[33m[UniversalGGUFLoader] Continuing in Text-Only mode...\033[0m")
                    chat_handler = None
            else:
                print(f"\033[33m[UniversalGGUFLoader] CLIP model not found: {clip_model}\033[0m")

        # [Auto-Detect Chat Format]
        # é’ˆå¯¹ Qwen ç­‰æ¨¡å‹ï¼Œè‡ªåŠ¨åº”ç”¨ chatml æ ¼å¼ï¼Œé¿å… llama-cpp-python çŒœé”™ã€‚
        # è¿™é‡Œè¿›è¡Œç®€å•çš„æ–‡ä»¶åå¯å‘å¼æ£€æµ‹ã€‚
        chat_format = None
        model_name = os.path.basename(model_path).lower()
        
        if "qwen" in model_name:
            chat_format = "chatml"
            print(f"\033[36m[UniversalGGUFLoader] Auto-detected Qwen model. Enforcing chat_format='chatml'.\033[0m")
        elif "llama-3" in model_name or "llama3" in model_name:
             chat_format = "llama-3"
        elif "vicuna" in model_name:
             chat_format = "vicuna"
        
        # å®ä¾‹åŒ–æ¨¡å‹
        model = Llama(
            model_path=model_path, 
            chat_handler=chat_handler,
            n_gpu_layers=n_gpu_layers, 
            n_ctx=n_ctx, 
            n_batch=512,
            chat_format=chat_format # æ³¨å…¥è‡ªåŠ¨è¯†åˆ«çš„æ ¼å¼
        )
        # æ ‡è®°æ˜¯å¦åŠ è½½äº† CLIPï¼Œä¾› Chat èŠ‚ç‚¹å‚è€ƒ
        model._loaded_clip_path = folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None
        # [Smart Vision Check] æ ‡è®°æ¨¡å‹æ˜¯å¦æ‹¥æœ‰æœ‰æ•ˆçš„ Vision Handler
        # è¿™å…è®¸ Chat èŠ‚ç‚¹åœ¨ç”¨æˆ·è¯¯è¿å›¾ç‰‡ä½†ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼ï¼Œé¿å…æŠ¥é”™ã€‚
        model._has_vision_handler = chat_handler is not None
        # [Model Name] è®°å½•æ¨¡å‹æ–‡ä»¶åï¼Œç”¨äºåç»­çš„æ™ºèƒ½åˆ¤æ–­
        model._model_filename = os.path.basename(model_path)
        # [Smart Detection] Check if model is Qwen-based (for special prompt handling)
        model._is_qwen = "qwen" in os.path.basename(model_path).lower()
        
        # [Auto-Reload Support] Save init params to allow Chat node to reload the model if closed
        model._init_params = {
            "model_path": model_path,
            "n_gpu_layers": n_gpu_layers,
            "n_ctx": n_ctx,
            "n_batch": 512,
            "chat_format": chat_format,
            "clip_path": folder_paths.get_full_path("llm", clip_model) if clip_model != "None" else None,
            "verbose": False
        }
        
        return (model,)

# ==========================================================
# 3. UniversalAIChat (NEW - Formerly LH_LlamaInstruct)
# ==========================================================
# PROJECT: UniversalAIChat
# LOGIC DEFINITION:
#   - Advanced version of UniversalAIChat (Replaces old Logic)
#   - Supports GBNF Grammar for structured output
#   - Supports Advanced Samplers (Mirostat, Min-P)
# ==========================================================
class UniversalAIChat:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("LLM_MODEL",), 
                "user_material": ("STRING", {"multiline": True, "default": DEFAULT_USER_MATERIAL}), 
                "instruction": ("STRING", {"multiline": True, "default": DEFAULT_INSTRUCTION}),
                "chat_mode": (["Enhance_Prompt", "Debug_Chat"],),
                "enable_tag": ("BOOLEAN", {"default": False, "label_on": "Enable Tags", "label_off": "Disable Tags"}),
                "enable_filename": ("BOOLEAN", {"default": False, "label_on": "Enable Filename", "label_off": "Disable Filename"}),
                "enable_cot": ("BOOLEAN", {"default": False, "label_on": "Enable Thinking (CoT)", "label_off": "Disable Thinking"}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.01}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.01}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "release_vram": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "image": ("IMAGE",),
                "min_p": ("FLOAT", {"default": 0.05, "min": 0.0, "max": 1.0, "step": 0.01}),
                "mirostat_mode": ("INT", {"default": 0, "min": 0, "max": 2}),
                "mirostat_tau": ("FLOAT", {"default": 5.0, "min": 0.0, "max": 10.0, "step": 0.1}),
                "mirostat_eta": ("FLOAT", {"default": 0.1, "min": 0.0, "max": 1.0, "step": 0.01}),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("prompt", "tags", "filename", "raw_output")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes"

    # å¼ºåˆ¶æ¯æ¬¡è¿è¡Œ (Force Execution)
    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def _build_grammar(self, enable_tag, enable_filename):
        """
        Builds a GBNF grammar string based on enabled features.
        """
        # GBNF Definition
        # root ::= description tags? filename?
        # description ::= "**description**:" space text
        # tags ::= "**tags**:" space text
        # filename ::= "**filename**:" space text
        
        # Basic text definition (allows newlines)
        # We need to be careful not to consume the next keyword.
        # But GBNF for "everything until keyword" is tricky.
        # Simple approach: standard text.
        
        gbnf = r"""
root ::= description
"""
        if enable_tag:
            gbnf += " tags"
        if enable_filename:
            gbnf += " filename"
            
        gbnf += r"""
description ::= "**description**:" "\n" content
tags ::= "**tags**:" "\n" tag_content
filename ::= "**filename**:" "\n" filename_content

content ::= [^#*]+ 
tag_content ::= [^#*]+
filename_content ::= "[" [a-zA-Z0-9_]+ "]"

"""
        # Note: The regex [^#*]+ is a simplification. 
        # Ideally we want "anything that doesn't look like a start tag".
        # For now, let's try a simpler approach without strict GBNF first?
        # Or use a very loose GBNF that just mandates the headers.
        
        # Let's try a structure that enforces the headers but allows free text in between.
        
        grammar_str = r"""
root ::= description
"""
        if enable_tag:
            grammar_str += " tags"
        if enable_filename:
            grammar_str += " filename"
            
        grammar_str += r"""
description ::= "**description**:\n" text
tags ::= "**tags**:\n" text
filename ::= "**filename**:\n" filename_pattern

text ::= ( [^#] | "#" [^*] )+ 
filename_pattern ::= "[" [a-zA-Z0-9_]+ "]"
"""
        return None
    
    def chat(self, model, user_material, instruction, chat_mode, enable_tag, enable_filename, enable_cot, max_tokens, temperature, repetition_penalty, seed, release_vram, min_p=0.05, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, image=None):
        # 0. åŸºç¡€é˜²å¾¡æ€§å¤„ç† (Defensive Check)
        if user_material is None: user_material = ""
        if instruction is None: instruction = ""

        # Ensure model is loaded
        if model is None:
             raise ValueError("Model is not loaded.")
        
        # Check and Reload if needed
        # Priority: Check _is_closed flag first
        need_reload = False
        if getattr(model, '_is_closed', False):
            need_reload = True
        else:
            try:
                 # Check if model context is valid
                 _ = model.n_ctx() 
            except:
                 need_reload = True

        if need_reload:
             if hasattr(model, '_init_params'):
                 print("\033[33m[UniversalAIChat] Model is closed or invalid. Reloading...\033[0m")
                 from llama_cpp import Llama
                 init_p = model._init_params
                 
                 # Re-instantiate model locally
                 try:
                     model = Llama(
                         model_path=init_p["model_path"],
                         n_gpu_layers=init_p["n_gpu_layers"],
                         n_ctx=init_p["n_ctx"],
                         n_batch=init_p["n_batch"],
                         chat_format=init_p["chat_format"],
                         verbose=init_p["verbose"],
                     )
                     # Restore attributes
                     model._init_params = init_p
                     model._loaded_clip_path = init_p.get("clip_path")
                     model._has_vision_handler = False 
                     model._model_filename = os.path.basename(init_p["model_path"])
                     model._is_closed = False # Reset flag for the new instance
                     
                     # Restore Vision Handler if needed
                     if model._loaded_clip_path:
                         try:
                            from llama_cpp.llama_chat_format import Llava15ChatHandler
                            clip_path = model._loaded_clip_path
                            if clip_path:
                                chat_handler = Llava15ChatHandler(clip_model_path=clip_path)
                                model.chat_handler = chat_handler
                                model._has_vision_handler = True
                         except:
                            pass
                 except Exception as e:
                     print(f"\033[31m[UniversalAIChat] Reload failed: {e}\033[0m")
                     raise ValueError(f"Model reload failed: {e}")
             else:
                 pass # Cannot reload, hope for the best
        
        # ==========================================================
        # 1. æ¨¡å¼åˆ¤å®šä¸é»˜è®¤æŒ‡ä»¤å®šä¹‰ (Mode Determination & Defaults)
        # ==========================================================
        
        # Widget Default Value (è§†ä¸ºâ€œç©ºâ€)
        WIDGET_DEFAULT_SC = ""

        # Mode Logic
        # Priority: Image > Enhance > Debug
        is_vision_task = image is not None
        current_mode = "VISION" if is_vision_task else chat_mode # "Enhance_Prompt" or "Debug_Chat"
        
        # Check SC status
        sc_stripped = instruction.strip()
        is_sc_empty = (not sc_stripped) or (sc_stripped == WIDGET_DEFAULT_SC.strip())
        
        # Prepare Variables
        final_system_command = instruction
        final_user_content = "" # For text part
        apply_template = False
        
        # Mode Specific Logic
        if is_vision_task:
            if not getattr(model, '_has_vision_handler', False):
                 err_msg = "[SYSTEM ERROR] Vision Task requested but no Vision Handler (CLIP/MMProj) is loaded.\nPlease make sure you selected a CLIP/Vision model in the Loader node."
                 print(f"\033[31m[{datetime.now().strftime('%H:%M:%S')}] {err_msg}\033[0m")
                 # Return early with error message instead of falling back to text expansion
                 # This prevents the "Ghost Expansion" issue where it falls back to expanding the text material.
                 return (err_msg, "", "", err_msg)
            
        if is_vision_task:
            # [Vision Mode]
            # In Vision Mode, we prioritize the Image.
            # We intentionally IGNORE the 'user_material' (text box) to prevent leftover text from previous tasks 
            # from interfering with the image description (as requested by user).
            # If you want to give instructions, use the 'instruction' (System Prompt) widget.
            
            if is_sc_empty:
                final_system_command = FALLBACK_VISION
            else:
                final_system_command = instruction
            
            final_user_content = "" # Explicitly clear text input
            apply_template = True
            
        elif current_mode == "Enhance_Prompt":
            # [Mode 2: Prompt Enhance]
            final_user_content = f"{LABEL_USER_INPUT}\n{user_material}"
            
            if is_sc_empty:
                final_system_command = FALLBACK_ENHANCE
            else:
                final_system_command = instruction
                
            apply_template = True

            
        elif current_mode == "Debug_Chat":
            # [Mode 3: Debug]
            final_user_content = user_material

            if is_sc_empty:
                final_system_command = FALLBACK_DEBUG
            else:
                final_system_command = instruction
            
            enable_tag = False
            enable_filename = False
            enable_cot = True 
            apply_template = False
            
        # ==========================================================
        # 2. æ¨¡æ¿æ„å»º (Template Construction)
        # ==========================================================
        template_instructions = ""
        needs_structure = enable_tag or enable_filename
        
        if apply_template:
            rules = []
            rules.extend(CONSTRAINT_NO_REPEAT)

            if not enable_cot:
                rules.extend(CONSTRAINT_NO_COT)
            else:
                rules.extend(CONSTRAINT_ALLOW_COT)

            if enable_tag:
                rules.append(PROMPT_TAGS)
            
            if enable_filename:
                rules.append(PROMPT_FILENAME)

            strict_constraints = CONSTRAINT_HEADER
            for i, rule in enumerate(rules, 1):
                strict_constraints += f"{i}. {rule}\n"
            
            output_order = [TRIGGER_ORDER_DESC]
            if enable_tag:
                output_order.append(TRIGGER_ORDER_TAGS)
            if enable_filename:
                output_order.append(TRIGGER_ORDER_FILENAME)
            
            start_sequence = f"{TRIGGER_PREFIX}{chr(10).join(output_order)}{TRIGGER_SUFFIX}"
            strict_constraints += start_sequence
            template_instructions += strict_constraints
            
        # ==========================================================
        # 3. æ¶ˆæ¯ç»„è£… (Message Assembly)
        # ==========================================================
        messages = []
        if final_system_command:
            messages.append({"role": "system", "content": final_system_command})
    
        # 3.2 User Message
        if is_vision_task:
            # [Vision Mode]
            i = 255. * image[0].cpu().numpy()
            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
            
            if img.mode == "RGBA":
                background = Image.new("RGB", img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[3]) 
                img = background
            elif img.mode != "RGB":
                img = img.convert("RGB")
                
            buffered = BytesIO()
            max_dimension = 1536
            if max(img.size) > max_dimension:
                scale_factor = max_dimension / max(img.size)
                new_size = (int(img.size[0] * scale_factor), int(img.size[1] * scale_factor))
                img = img.resize(new_size, Image.Resampling.LANCZOS)
                # print(f"\033[36m[UniversalAIChat] Image Resized to {img.size}\033[0m")

            img.save(buffered, format="JPEG", quality=95) 
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            user_text_content = f"{final_user_content}{template_instructions}"
            
            user_content_list = [
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}},
                {"type": "text", "text": user_text_content}
            ]
            
            messages.append({"role": "user", "content": user_content_list})
            display_up = f"[IMAGE]\n{LABEL_USER_INPUT}\n{user_material}"
            
        else:
            # [Text Mode]
            final_text_content = f"{final_user_content}{template_instructions}"
            messages.append({"role": "user", "content": final_text_content})
            display_up = f"{LABEL_USER_INPUT}\n{user_material}"

        # ==========================================================
        # 4. æ¨ç†æ‰§è¡Œ (Inference Execution)
        # ==========================================================
        
        try:
            stop_tokens = ["<|im_end|>", "<|endoftext|>", "User:", "\nUser:"] 
            safe_temperature = min(max(temperature, 0.0), 2.0)
            grammar = None
            if apply_template and needs_structure:
                 pass

            output = None
            full_res = ""
            finish_reason = "unknown"
            usage = {}

            max_attempts = 2 if is_vision_task else 1
            attempt = 0
            sampler_used = "text-default" if not is_vision_task else "vision-advanced"

            # [HOTFIX] Temporary disable vision handler for text-only tasks
            # This prevents "Failed to load mtmd context" errors when mmproj is selected but no image provided.
            original_handler = None
            if not is_vision_task and hasattr(model, 'chat_handler'):
                original_handler = model.chat_handler
                model.chat_handler = None

            try:
                while attempt < max_attempts:
                    attempt += 1

                    if is_vision_task:
                        if attempt == 1:
                            eff_min_p = min_p
                            eff_mirostat_mode = mirostat_mode
                            sampler_used = "vision-advanced"
                        else:
                            eff_min_p = 0.0
                            eff_mirostat_mode = 0
                            sampler_used = "vision-safe"
                        eff_mirostat_tau = mirostat_tau
                        eff_mirostat_eta = mirostat_eta
                    else:
                        eff_min_p = min_p
                        eff_mirostat_mode = mirostat_mode
                        eff_mirostat_tau = mirostat_tau
                        eff_mirostat_eta = mirostat_eta
                        sampler_used = "text-default"

                    local_error = None
                    try:
                        # [Fix] top_p should not be assigned min_p value. We use default top_p=0.9 and let min_p handle truncation.
                        output = model.create_chat_completion(
                            messages=messages, 
                            max_tokens=max_tokens, 
                            temperature=safe_temperature, 
                            repeat_penalty=repetition_penalty, 
                            top_p=0.9, 
                            min_p=eff_min_p,
                            mirostat_mode=eff_mirostat_mode,
                            mirostat_tau=eff_mirostat_tau,
                            mirostat_eta=eff_mirostat_eta,
                            seed=seed,
                            stop=stop_tokens,
                            grammar=grammar
                        )
                    except Exception as e_inner:
                        local_error = e_inner

                    if local_error is not None:
                        # If error is about vision/embedding, try fallback
                        err_str = str(local_error).lower()
                        if is_vision_task and attempt < max_attempts:
                             print(f"\033[33m[UniversalAIChat] Vision attempt {attempt} failed ({err_str}). Retrying with SAFE samplers...\033[0m")
                             continue
                        else:
                            raise local_error
                    else:
                        # Success
                        break
            finally:
                # Restore original handler if it was removed
                if original_handler is not None and hasattr(model, 'chat_handler'):
                    model.chat_handler = original_handler

            if not output or 'choices' not in output or not output['choices']:
                 raise ValueError("Empty response from model.")
            full_res = output['choices'][0]['message']['content']
            finish_reason = output['choices'][0].get('finish_reason', 'unknown')
            usage = output.get('usage', {})

            if finish_reason == 'length':
                full_res += "\n\n[SYSTEM: Output Truncated. Max Tokens Reached.]"
            
            # [Post-Processing]
            if full_res:
                 for token in ["[/INST]", "[INST]", "<|im_end|>", "<|endoftext|>", "<|im_start|>", "User:"]:
                     full_res = full_res.replace(token, "")
            
        except Exception as e:
            error_msg = str(e)
            full_res = f"Error: {error_msg}"
            # print(f"\033[31m[UniversalAIChat] Generation Error: {error_msg}\033[0m")
            # Minimal error logging
            if "No KV slot available" in error_msg:
                 full_res += "\n\n[SYSTEM ERROR]: Context Window Full (n_ctx too small). Please increase 'n_ctx' in the Loader node."

        # 4. è¾“å‡ºè§£æ (Output Parsing)
        # ==========================================================
        
        # Log to Console (Raw Output)
        if is_vision_task:
            user_log = f"ğŸ›¡ï¸ [System Instruction]:\n{final_system_command}\n\n[IMAGE INPUT PROVIDED]\n(Text Input Ignored in Vision Mode)\nOriginal Text: {user_material}\n\n[Template Constraints]:\n{template_instructions}"
        else:
            user_log = f"ğŸ›¡ï¸ [System Instruction]:\n{final_system_command}\n\n{LABEL_USER_INPUT}\n{user_material}\n\n[Template Constraints]:\n{template_instructions}"
            
        debug_meta = f"[MODE: {current_mode}, SAMPLER: {sampler_used}, Temp: {safe_temperature:.2f}, Min_P: {eff_min_p:.2f}]"
        raw_output = f"User: {user_log}\n\nAI:\n{full_res}\n\n{debug_meta}"

        # Release VRAM if requested
        if release_vram:
             try:
                if hasattr(model, "close"):
                    model.close()
                print("\033[33m[UniversalAIChat] Model VRAM Released (Closed).\033[0m")
             except:
                pass
             model._is_closed = True


        # 5. åˆ†å‰²é€»è¾‘ (Simple Splitter)
        clean_res = re.sub(r'<think>.*?</think>', '', full_res, flags=re.DOTALL).strip()
        if '<think>' in clean_res:
            clean_res = clean_res.split('<think>')[0].strip()
            
        marker_desc = "**description**:"
        marker_tags = "**tags**:"
        marker_filename = "**filename**:"
        
        def get_pos(marker, text):
            # Find all occurrences
            matches = [m for m in re.finditer(re.escape(marker), text, re.IGNORECASE)]
            if not matches:
                return -1
            
            # Smart Filter: Ignore occurrences that look like Template Echo
            # Template usually looks like "**description**:\n[description]"
            valid_matches = []
            for m in matches:
                start = m.start()
                # Check next 20 chars for placeholder
                snippet = text[start + len(marker):start + len(marker) + 20]
                if "[description]" in snippet or "[tags]" in snippet or "[filename]" in snippet:
                    continue # Likely a prompt echo
                valid_matches.append(m)
            
            # If we filtered everything (or nothing left), try fallback to LAST match (assuming it's the output)
            if not valid_matches:
                 # If all matches looked like templates, maybe the model really outputted the template?
                 # Or maybe we were too strict. 
                 # But if we have multiple matches, the last one is most likely the AI output.
                 if len(matches) > 1:
                     return matches[-1].start()
                 return matches[0].start()
            
            # If we have valid matches, take the FIRST one (Option 1)
            return valid_matches[0].start()
            
        pos_desc = get_pos(marker_desc, clean_res)
        pos_tags = get_pos(marker_tags, clean_res)
        pos_filename = get_pos(marker_filename, clean_res)
        
        start_desc = 0
        if pos_desc != -1:
            start_desc = pos_desc + len(marker_desc)
            
        end_desc = len(clean_res)
        candidates = []
        if pos_tags != -1 and pos_tags > start_desc: candidates.append(pos_tags)
        if pos_filename != -1 and pos_filename > start_desc: candidates.append(pos_filename)
        
        if candidates:
            end_desc = min(candidates)
            
        out_desc = clean_res[start_desc:end_desc].strip()
        
        out_tags = ""
        if enable_tag:
            if pos_tags != -1:
                start_tags = pos_tags + len(marker_tags)
                end_tags = len(clean_res)
                if pos_filename != -1 and pos_filename > start_tags:
                    end_tags = pos_filename
                raw_tags = clean_res[start_tags:end_tags].strip()
                out_tags = raw_tags.replace("\n", ",")
        else:
            out_tags = ""
            
        out_filename = ""
        if enable_filename:
            if pos_filename != -1:
                 start_fn = pos_filename + len(marker_filename)
                 raw_fn = clean_res[start_fn:].strip()
                 m = re.search(r'\[(.*?)\]', raw_fn)
                 if m:
                     out_filename = m.group(1)
                 else:
                     out_filename = raw_fn.split('\n')[0]
        else:
            out_filename = ""

        # ==========================================================
        # 6. è¾“å‡ºç»“æœ (Return)
        # ==========================================================
        return (out_desc, out_tags, out_filename, raw_output)


# ==========================================================
# 4. UniversalAIChat_Legacy (Old AIChat Node - Shelved)
# ==========================================================
class UniversalAIChat_Legacy:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("LLM_MODEL",), 
                "user_material": ("STRING", {"multiline": True, "default": DEFAULT_USER_MATERIAL}), 
                "instruction": ("STRING", {"multiline": True, "default": DEFAULT_INSTRUCTION}),
                "chat_mode": (["Enhance_Prompt", "Debug_Chat"],),
                "enable_tag": ("BOOLEAN", {"default": False, "label_on": "Enable Tags", "label_off": "Disable Tags"}),
                "enable_filename": ("BOOLEAN", {"default": False, "label_on": "Enable Filename", "label_off": "Disable Filename"}),
                "enable_cot": ("BOOLEAN", {"default": False, "label_on": "Enable Thinking (CoT)", "label_off": "Disable Thinking"}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 8192}),
                "temperature": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 2.0, "step": 0.01}),
                "repetition_penalty": ("FLOAT", {"default": 1.1, "min": 1.0, "max": 2.0, "step": 0.01}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "release_vram": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "image": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("prompt", "tags", "filename", "raw_output")
    FUNCTION = "chat"
    CATEGORY = "custom_nodes/MyLoraNodes/Legacy"

    @classmethod
    def IS_CHANGED(s, **kwargs):
        return float("nan")

    def chat(self, model, user_material, instruction, chat_mode, enable_tag, enable_filename, enable_cot, max_tokens, temperature, repetition_penalty, seed, release_vram, image=None):
        return ("Legacy Node - Shelved", "", "", "This node is deprecated. Please use the new UniversalAIChat node.")


# 4. å†å²ç›‘æ§èŠ‚ç‚¹ (æµæ°´çº¿æ’åº)
# ==========================================================
# PROJECT: LoraHelper_Monitor (History Viewer)
# MANDATORY UI ORDER (INPUT_TYPES):
#   1. raw_output (Raw Text Input)
#
# LOGIC DEFINITION:
#   - Maintains a rolling buffer of last 5 chat interactions
#   - Output 1: context (Raw text for LLM)
#   - UI Display: Formatted cards (Old -> New)
# ==========================================================
class LH_History_Monitor:
    def __init__(self):
        self.history = []

    @classmethod
    def INPUT_TYPES(s):
        return { 
            "required": { 
                "raw_input": ("STRING", {"forceInput": True}),
                "clear_history": ("BOOLEAN", {"default": False, "label_on": "Clear History", "label_off": "Keep History"})
            } 
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("context",)
    OUTPUT_NODE = True
    FUNCTION = "update"
    CATEGORY = "custom_nodes/MyLoraNodes"

    def update(self, raw_input, clear_history):
        # 0. Clear History Check
        if clear_history:
            self.history = []
            # We still process the current input, but it will be the ONLY item in history.
            print("\033[36m[LH_History_Monitor] History Cleared by User.\033[0m")
        # 1. è§£æè¾“å…¥ (æ”¯æŒ JSON æˆ– çº¯æ–‡æœ¬)
        import json
        user_msg = ""
        ai_msg = ""
        
        # å°è¯•è§£æç‰¹å®šæ ¼å¼ "User: ... \nAI: ..."
        if isinstance(raw_input, str) and raw_input.startswith("User:"):
             # ä½¿ç”¨ split åˆ†å‰²ï¼Œæ³¨æ„åªåˆ†å‰²ç¬¬ä¸€ä¸ª "\nAI: "
             parts = raw_input.split("\nAI: ", 1)
             if len(parts) == 2:
                 user_msg = parts[0][5:].strip() # å»æ‰ "User: "
                 ai_msg = parts[1].strip()
             else:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        else:
            try:
                data = json.loads(raw_input)
                if isinstance(data, dict):
                    user_msg = data.get("user", "")
                    ai_msg = data.get("ai", "")
                else:
                    user_msg = "Raw Input"
                    ai_msg = str(raw_input)
            except:
                 user_msg = "Raw Input"
                 ai_msg = str(raw_input)
        
        # 2. æ›´æ–°å†å² (å»é‡)
        # æ„é€ ä¸€ä¸ªç»“æ„åŒ–å¯¹è±¡å­˜å‚¨
        new_entry = {"user": user_msg, "ai": ai_msg}
        
        # ç®€å•å»é‡ï¼šæ£€æŸ¥ä¸Šä¸€æ¡æ˜¯å¦å®Œå…¨ä¸€è‡´
        if self.history:
            last = self.history[-1]
            if last["user"] == user_msg and last["ai"] == ai_msg:
                pass # é‡å¤ï¼Œå¿½ç•¥
            else:
                self.history.append(new_entry)
        else:
            self.history.append(new_entry)
            
        # ä¿æŒ 5 è½®
        if len(self.history) > 5:
            self.history.pop(0)

        # 3. æ„é€  Context (ç”¨äºå›ä¼ ç»™ Chat)
        # æ ¼å¼ï¼šRound X User: ... \n Round X AI: ...
        context_parts = []
        for i, h in enumerate(self.history):
            context_parts.append(f"Round {i+1} User: {h['user']}")
            context_parts.append(f"Round {i+1} AI: {h['ai']}")
        context = "\n\n".join(context_parts)

        # 4. æ„é€  UI æ˜¾ç¤º â€”â€” å…³é”®ä¿®æ”¹ï¼šæ‹†åˆ†æˆå¤šä¸ªçŸ­æ–‡æœ¬å—
        ui_text = []
        ui_text.append("â•â•â•â•â•â•â•â•â• ğŸ‘€ Visual History (Latest 5 Rounds) â•â•â•â•â•â•â•â•â•\n")
        
        for i, h in enumerate(reversed(self.history)): # æœ€æ–°è½®åœ¨ä¸Š
            idx = len(self.history) - i
            ui_text.append(f"ğŸ”» Round {idx} â€” User è¾“å…¥")
            ui_text.append(h["user"] or "(ç©º)")  # å•ç‹¬ä¸€å—ï¼Œç”¨æˆ·è¾“å…¥
            
            ui_text.append(f"ğŸ”¹ Round {idx} â€” AI è¾“å‡º")
            ui_text.append(h["ai"] or "(ç©º)")    # å•ç‹¬ä¸€å—ï¼ŒAIè¾“å‡º
            
            ui_text.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")  # åˆ†éš”çº¿

        # å¦‚æœå†å²ä¸ºç©º
        if len(ui_text) <= 1:
             ui_text.append("ï¼ˆæš‚æ— å¯¹è¯å†å²ï¼‰")
        
        return {"ui": {"text": ui_text}, "result": (context,)}
